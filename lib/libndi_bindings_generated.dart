// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
import 'dart:ffi' as ffi;

/// Bindings for `src/libndi_bindings.h`.
///
/// Regenerate bindings with `dart run ffigen --config ffigen.yaml`.
///
class LibndiBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  LibndiBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  LibndiBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  /// #define INFINITE INFINITE
  late final ffi.Pointer<ffi.Uint32> _INFINITE =
      _lookup<ffi.Uint32>('INFINITE');

  int get INFINITE => _INFINITE.value;

  set INFINITE(int value) => _INFINITE.value = value;

  /// When you specify this as a timecode, the timecode will be synthesized for you. This may be used when
  /// sending video, audio or metadata. If you never specify a timecode at all, asking for each to be
  /// synthesized, then this will use the current system time as the starting timecode and then generate
  /// synthetic ones, keeping your streams exactly in sync as long as the frames you are sending do not deviate
  /// from the system time in any meaningful way. In practice this means that if you never specify timecodes
  /// that they will always be generated for you correctly. Timecodes coming from different senders on the same
  /// machine will always be in sync with each other when working in this way. If you have NTP installed on your
  /// local network, then streams can be synchronized between multiple machines with very high precision.
  ///
  /// If you specify a timecode at a particular frame (audio or video), then ask for all subsequent ones to be
  /// synthesized. The subsequent ones will be generated to continue this sequence maintaining the correct
  /// relationship both the between streams and samples generated, avoiding them deviating in time from the
  /// timecode that you specified in any meaningful way.
  ///
  /// If you specify timecodes on one stream (e.g. video) and ask for the other stream (audio) to be
  /// synthesized, the correct timecodes will be generated for the other stream and will be synthesize exactly
  /// to match (they are not quantized inter-streams) the correct sample positions.
  ///
  /// When you send metadata messages and ask for the timecode to be synthesized, then it is chosen to match the
  /// closest audio or video frame timecode so that it looks close to something you might want ... unless there
  /// is no sample that looks close in which a timecode is synthesized from the last ones known and the time
  /// since it was sent.
  late final ffi.Pointer<ffi.Int64> _NDIlib_send_timecode_synthesize =
      _lookup<ffi.Int64>('NDIlib_send_timecode_synthesize');

  int get NDIlib_send_timecode_synthesize =>
      _NDIlib_send_timecode_synthesize.value;

  set NDIlib_send_timecode_synthesize(int value) =>
      _NDIlib_send_timecode_synthesize.value = value;

  /// If the time-stamp is not available (i.e. a version of a sender before v2.5)
  late final ffi.Pointer<ffi.Int64> _NDIlib_recv_timestamp_undefined =
      _lookup<ffi.Int64>('NDIlib_recv_timestamp_undefined');

  int get NDIlib_recv_timestamp_undefined =>
      _NDIlib_recv_timestamp_undefined.value;

  set NDIlib_recv_timestamp_undefined(int value) =>
      _NDIlib_recv_timestamp_undefined.value = value;

  /// This is not actually required, but will start and end the libraries which might get you slightly better
  /// performance in some cases. In general it is more "correct" to call these although it is not required.
  /// There is no way to call these that would have an adverse impact on anything (even calling destroy before
  /// you've deleted all your objects). This will return false if the CPU is not sufficiently capable to run
  /// NDILib currently NDILib requires SSE4.2 instructions (see documentation). You can verify a specific CPU
  /// against the library with a call to NDIlib_is_supported_CPU().
  bool NDIlib_initialize() {
    return _NDIlib_initialize() != 0;
  }

  late final _NDIlib_initializePtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function()>>('NDIlib_initialize');
  late final _NDIlib_initialize =
      _NDIlib_initializePtr.asFunction<int Function()>();

  void NDIlib_destroy() {
    return _NDIlib_destroy();
  }

  late final _NDIlib_destroyPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('NDIlib_destroy');
  late final _NDIlib_destroy = _NDIlib_destroyPtr.asFunction<void Function()>();

  ffi.Pointer<ffi.Int8> NDIlib_version() {
    return _NDIlib_version();
  }

  late final _NDIlib_versionPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Int8> Function()>>(
          'NDIlib_version');
  late final _NDIlib_version =
      _NDIlib_versionPtr.asFunction<ffi.Pointer<ffi.Int8> Function()>();

  /// Recover whether the current CPU in the system is capable of running NDILib.
  bool NDIlib_is_supported_CPU() {
    return _NDIlib_is_supported_CPU() != 0;
  }

  late final _NDIlib_is_supported_CPUPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function()>>(
          'NDIlib_is_supported_CPU');
  late final _NDIlib_is_supported_CPU =
      _NDIlib_is_supported_CPUPtr.asFunction<int Function()>();

  /// ***********************************************************************************************************
  /// Create a new finder instance. This will return NULL if it fails.
  NDIlib_find_instance_t NDIlib_find_create_v2(
    ffi.Pointer<NDIlib_find_create_t> p_create_settings,
  ) {
    return _NDIlib_find_create_v2(
      p_create_settings,
    );
  }

  late final _NDIlib_find_create_v2Ptr = _lookup<
      ffi.NativeFunction<
          NDIlib_find_instance_t Function(
              ffi.Pointer<NDIlib_find_create_t>)>>('NDIlib_find_create_v2');
  late final _NDIlib_find_create_v2 = _NDIlib_find_create_v2Ptr.asFunction<
      NDIlib_find_instance_t Function(ffi.Pointer<NDIlib_find_create_t>)>();

  /// This will destroy an existing finder instance.
  void NDIlib_find_destroy(
    NDIlib_find_instance_t p_instance,
  ) {
    return _NDIlib_find_destroy(
      p_instance,
    );
  }

  late final _NDIlib_find_destroyPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_find_instance_t)>>(
          'NDIlib_find_destroy');
  late final _NDIlib_find_destroy = _NDIlib_find_destroyPtr.asFunction<
      void Function(NDIlib_find_instance_t)>();

  /// This function will recover the current set of sources (i.e. the ones that exist right this second). The
  /// char* memory buffers returned in NDIlib_source_t are valid until the next call to
  /// NDIlib_find_get_current_sources or a call to NDIlib_find_destroy. For a given NDIlib_find_instance_t, do
  /// not call NDIlib_find_get_current_sources asynchronously.
  ffi.Pointer<NDIlib_source_t> NDIlib_find_get_current_sources(
    NDIlib_find_instance_t p_instance,
    ffi.Pointer<ffi.Uint32> p_no_sources,
  ) {
    return _NDIlib_find_get_current_sources(
      p_instance,
      p_no_sources,
    );
  }

  late final _NDIlib_find_get_current_sourcesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<NDIlib_source_t> Function(NDIlib_find_instance_t,
              ffi.Pointer<ffi.Uint32>)>>('NDIlib_find_get_current_sources');
  late final _NDIlib_find_get_current_sources =
      _NDIlib_find_get_current_sourcesPtr.asFunction<
          ffi.Pointer<NDIlib_source_t> Function(
              NDIlib_find_instance_t, ffi.Pointer<ffi.Uint32>)>();

  /// This will allow you to wait until the number of online sources have changed.
  bool NDIlib_find_wait_for_sources(
    NDIlib_find_instance_t p_instance,
    int timeout_in_ms,
  ) {
    return _NDIlib_find_wait_for_sources(
          p_instance,
          timeout_in_ms,
        ) !=
        0;
  }

  late final _NDIlib_find_wait_for_sourcesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_find_instance_t,
              ffi.Uint32)>>('NDIlib_find_wait_for_sources');
  late final _NDIlib_find_wait_for_sources = _NDIlib_find_wait_for_sourcesPtr
      .asFunction<int Function(NDIlib_find_instance_t, int)>();

  /// **************************************************************************************************************************
  /// Create a new receiver instance. This will return NULL if it fails. If you create this with the default
  /// settings (NULL) then it will automatically determine a receiver name.
  NDIlib_recv_instance_t NDIlib_recv_create_v3(
    ffi.Pointer<NDIlib_recv_create_v3_t> p_create_settings,
  ) {
    return _NDIlib_recv_create_v3(
      p_create_settings,
    );
  }

  late final _NDIlib_recv_create_v3Ptr = _lookup<
      ffi.NativeFunction<
          NDIlib_recv_instance_t Function(
              ffi.Pointer<NDIlib_recv_create_v3_t>)>>('NDIlib_recv_create_v3');
  late final _NDIlib_recv_create_v3 = _NDIlib_recv_create_v3Ptr.asFunction<
      NDIlib_recv_instance_t Function(ffi.Pointer<NDIlib_recv_create_v3_t>)>();

  /// This will destroy an existing receiver instance.
  void NDIlib_recv_destroy(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_destroy(
      p_instance,
    );
  }

  late final _NDIlib_recv_destroyPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_destroy');
  late final _NDIlib_recv_destroy = _NDIlib_recv_destroyPtr.asFunction<
      void Function(NDIlib_recv_instance_t)>();

  /// This function allows you to change the connection to another video source, you can also disconnect it by
  /// specifying a NULL here. This allows you to preserve a receiver without needing to.
  void NDIlib_recv_connect(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_source_t> p_src,
  ) {
    return _NDIlib_recv_connect(
      p_instance,
      p_src,
    );
  }

  late final _NDIlib_recv_connectPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_source_t>)>>('NDIlib_recv_connect');
  late final _NDIlib_recv_connect = _NDIlib_recv_connectPtr.asFunction<
      void Function(NDIlib_recv_instance_t, ffi.Pointer<NDIlib_source_t>)>();

  /// This will allow you to receive video, audio and metadata frames. Any of the buffers can be NULL, in which
  /// case data of that type will not be captured in this call. This call can be called simultaneously on
  /// separate threads, so it is entirely possible to receive audio, video, metadata all on separate threads.
  /// This function will return NDIlib_frame_type_none if no data is received within the specified timeout and
  /// NDIlib_frame_type_error if the connection is lost. Buffers captured with this must be freed with the
  /// appropriate free function below.
  int NDIlib_recv_capture_v2(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_audio_data,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
    int timeout_in_ms,
  ) {
    return _NDIlib_recv_capture_v2(
      p_instance,
      p_video_data,
      p_audio_data,
      p_metadata,
      timeout_in_ms,
    );
  }

  late final _NDIlib_recv_capture_v2Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_video_frame_v2_t>,
              ffi.Pointer<NDIlib_audio_frame_v2_t>,
              ffi.Pointer<NDIlib_metadata_frame_t>,
              ffi.Uint32)>>('NDIlib_recv_capture_v2');
  late final _NDIlib_recv_capture_v2 = _NDIlib_recv_capture_v2Ptr.asFunction<
      int Function(
          NDIlib_recv_instance_t,
          ffi.Pointer<NDIlib_video_frame_v2_t>,
          ffi.Pointer<NDIlib_audio_frame_v2_t>,
          ffi.Pointer<NDIlib_metadata_frame_t>,
          int)>();

  /// This will allow you to receive video, audio and metadata frames. Any of the buffers can be NULL, in which
  /// case data of that type will not be captured in this call. This call can be called simultaneously on
  /// separate threads, so it is entirely possible to receive audio, video, metadata all on separate threads.
  /// This function will return NDIlib_frame_type_none if no data is received within the specified timeout and
  /// NDIlib_frame_type_error if the connection is lost. Buffers captured with this must be freed with the
  /// appropriate free function below.
  int NDIlib_recv_capture_v3(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
    ffi.Pointer<NDIlib_audio_frame_v3_t> p_audio_data,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
    int timeout_in_ms,
  ) {
    return _NDIlib_recv_capture_v3(
      p_instance,
      p_video_data,
      p_audio_data,
      p_metadata,
      timeout_in_ms,
    );
  }

  late final _NDIlib_recv_capture_v3Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_video_frame_v2_t>,
              ffi.Pointer<NDIlib_audio_frame_v3_t>,
              ffi.Pointer<NDIlib_metadata_frame_t>,
              ffi.Uint32)>>('NDIlib_recv_capture_v3');
  late final _NDIlib_recv_capture_v3 = _NDIlib_recv_capture_v3Ptr.asFunction<
      int Function(
          NDIlib_recv_instance_t,
          ffi.Pointer<NDIlib_video_frame_v2_t>,
          ffi.Pointer<NDIlib_audio_frame_v3_t>,
          ffi.Pointer<NDIlib_metadata_frame_t>,
          int)>();

  /// Free the buffers returned by capture for video
  void NDIlib_recv_free_video_v2(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
  ) {
    return _NDIlib_recv_free_video_v2(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_recv_free_video_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_recv_free_video_v2');
  late final _NDIlib_recv_free_video_v2 =
      _NDIlib_recv_free_video_v2Ptr.asFunction<
          void Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// Free the buffers returned by capture for audio
  void NDIlib_recv_free_audio_v2(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_audio_data,
  ) {
    return _NDIlib_recv_free_audio_v2(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_recv_free_audio_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_recv_free_audio_v2');
  late final _NDIlib_recv_free_audio_v2 =
      _NDIlib_recv_free_audio_v2Ptr.asFunction<
          void Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  /// Free the buffers returned by capture for audio
  void NDIlib_recv_free_audio_v3(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v3_t> p_audio_data,
  ) {
    return _NDIlib_recv_free_audio_v3(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_recv_free_audio_v3Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v3_t>)>>(
      'NDIlib_recv_free_audio_v3');
  late final _NDIlib_recv_free_audio_v3 =
      _NDIlib_recv_free_audio_v3Ptr.asFunction<
          void Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_audio_frame_v3_t>)>();

  /// Free the buffers returned by capture for metadata
  void NDIlib_recv_free_metadata(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_recv_free_metadata(
      p_instance,
      p_metadata,
    );
  }

  late final _NDIlib_recv_free_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_recv_free_metadata');
  late final _NDIlib_recv_free_metadata =
      _NDIlib_recv_free_metadataPtr.asFunction<
          void Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// This will free a string that was allocated and returned by NDIlib_recv (for instance the
  /// NDIlib_recv_get_web_control) function.
  void NDIlib_recv_free_string(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<ffi.Int8> p_string,
  ) {
    return _NDIlib_recv_free_string(
      p_instance,
      p_string,
    );
  }

  late final _NDIlib_recv_free_stringPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_recv_instance_t,
              ffi.Pointer<ffi.Int8>)>>('NDIlib_recv_free_string');
  late final _NDIlib_recv_free_string = _NDIlib_recv_free_stringPtr.asFunction<
      void Function(NDIlib_recv_instance_t, ffi.Pointer<ffi.Int8>)>();

  /// This function will send a meta message to the source that we are connected too. This returns FALSE if we
  /// are not currently connected to anything.
  bool NDIlib_recv_send_metadata(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_recv_send_metadata(
          p_instance,
          p_metadata,
        ) !=
        0;
  }

  late final _NDIlib_recv_send_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Uint8 Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_recv_send_metadata');
  late final _NDIlib_recv_send_metadata =
      _NDIlib_recv_send_metadataPtr.asFunction<
          int Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// Set the up-stream tally notifications. This returns FALSE if we are not currently connected to anything.
  /// That said, the moment that we do connect to something it will automatically be sent the tally state.
  bool NDIlib_recv_set_tally(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_tally_t> p_tally,
  ) {
    return _NDIlib_recv_set_tally(
          p_instance,
          p_tally,
        ) !=
        0;
  }

  late final _NDIlib_recv_set_tallyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_tally_t>)>>('NDIlib_recv_set_tally');
  late final _NDIlib_recv_set_tally = _NDIlib_recv_set_tallyPtr.asFunction<
      int Function(NDIlib_recv_instance_t, ffi.Pointer<NDIlib_tally_t>)>();

  /// Get the current performance structures. This can be used to determine if you have been calling
  /// NDIlib_recv_capture fast enough, or if your processing of data is not keeping up with real-time. The total
  /// structure will give you the total frame counts received, the dropped structure will tell you how many
  /// frames have been dropped. Either of these could be NULL.
  void NDIlib_recv_get_performance(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_recv_performance_t> p_total,
    ffi.Pointer<NDIlib_recv_performance_t> p_dropped,
  ) {
    return _NDIlib_recv_get_performance(
      p_instance,
      p_total,
      p_dropped,
    );
  }

  late final _NDIlib_recv_get_performancePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_recv_performance_t>,
                  ffi.Pointer<NDIlib_recv_performance_t>)>>(
      'NDIlib_recv_get_performance');
  late final _NDIlib_recv_get_performance =
      _NDIlib_recv_get_performancePtr.asFunction<
          void Function(
              NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_recv_performance_t>,
              ffi.Pointer<NDIlib_recv_performance_t>)>();

  /// This will allow you to determine the current queue depth for all of the frame sources at any time.
  void NDIlib_recv_get_queue(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_recv_queue_t> p_total,
  ) {
    return _NDIlib_recv_get_queue(
      p_instance,
      p_total,
    );
  }

  late final _NDIlib_recv_get_queuePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_recv_queue_t>)>>('NDIlib_recv_get_queue');
  late final _NDIlib_recv_get_queue = _NDIlib_recv_get_queuePtr.asFunction<
      void Function(
          NDIlib_recv_instance_t, ffi.Pointer<NDIlib_recv_queue_t>)>();

  /// Connection based metadata is data that is sent automatically each time a new connection is received. You
  /// queue all of these up and they are sent on each connection. To reset them you need to clear them all and
  /// set them up again.
  void NDIlib_recv_clear_connection_metadata(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_clear_connection_metadata(
      p_instance,
    );
  }

  late final _NDIlib_recv_clear_connection_metadataPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_clear_connection_metadata');
  late final _NDIlib_recv_clear_connection_metadata =
      _NDIlib_recv_clear_connection_metadataPtr.asFunction<
          void Function(NDIlib_recv_instance_t)>();

  /// Add a connection metadata string to the list of what is sent on each new connection. If someone is already
  /// connected then this string will be sent to them immediately.
  void NDIlib_recv_add_connection_metadata(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_recv_add_connection_metadata(
      p_instance,
      p_metadata,
    );
  }

  late final _NDIlib_recv_add_connection_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_recv_add_connection_metadata');
  late final _NDIlib_recv_add_connection_metadata =
      _NDIlib_recv_add_connection_metadataPtr.asFunction<
          void Function(
              NDIlib_recv_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// Is this receiver currently connected to a source on the other end, or has the source not yet been found or
  /// is no longer online. This will normally return 0 or 1.
  int NDIlib_recv_get_no_connections(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_get_no_connections(
      p_instance,
    );
  }

  late final _NDIlib_recv_get_no_connectionsPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_get_no_connections');
  late final _NDIlib_recv_get_no_connections =
      _NDIlib_recv_get_no_connectionsPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Get the URL that might be used for configuration of this input. Note that it might take a second or two
  /// after the connection for this value to be set. This function will return NULL if there is no web control
  /// user interface. You should call NDIlib_recv_free_string to free the string that is returned by this
  /// function. The returned value will be a fully formed URL, for instance "http://10.28.1.192/configuration/".
  /// To avoid the need to poll this function, you can know when the value of this function might have changed
  /// when the NDILib_recv_capture* call would return NDIlib_frame_type_status_change.
  ffi.Pointer<ffi.Int8> NDIlib_recv_get_web_control(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_get_web_control(
      p_instance,
    );
  }

  late final _NDIlib_recv_get_web_controlPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Int8> Function(
              NDIlib_recv_instance_t)>>('NDIlib_recv_get_web_control');
  late final _NDIlib_recv_get_web_control = _NDIlib_recv_get_web_controlPtr
      .asFunction<ffi.Pointer<ffi.Int8> Function(NDIlib_recv_instance_t)>();

  /// Has this receiver got PTZ control. Note that it might take a second or two after the connection for this
  /// value to be set. To avoid the need to poll this function, you can know when the value of this function
  /// might have changed when the NDILib_recv_capture* call would return NDIlib_frame_type_status_change
  bool NDIlib_recv_ptz_is_supported(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_is_supported(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_is_supportedPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_is_supported');
  late final _NDIlib_recv_ptz_is_supported = _NDIlib_recv_ptz_is_supportedPtr
      .asFunction<int Function(NDIlib_recv_instance_t)>();

  /// Has this receiver got recording control. Note that it might take a second or two after the connection for
  /// this value to be set. To avoid the need to poll this function, you can know when the value of this
  /// function might have changed when the NDILib_recv_capture* call would return NDIlib_frame_type_status_change.
  ///
  /// Note on deprecation of this function:
  /// NDI version 4 includes the native ability to record all NDI streams using an external application that
  /// is provided with the SDK. This is better in many ways than the internal recording support which only
  /// ever supported remotely recording systems and NDI|HX. This functionality will be supported in the SDK
  /// for some time although we are recommending that you use the newer support which is more feature rich and
  /// supports the recording of all stream types, does not take CPU time to record NDI sources (it does not
  /// require any type of re-compression since it can just store the data in the file), it will synchronize
  /// all recorders on a system (and cross systems if NTP clock locking is used).
  bool NDIlib_recv_recording_is_supported(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_recording_is_supported(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_is_supportedPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_recording_is_supported');
  late final _NDIlib_recv_recording_is_supported =
      _NDIlib_recv_recording_is_supportedPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// PTZ Controls
  /// Zoom to an absolute value.
  /// zoom_value = 0.0 (zoomed in) ... 1.0 (zoomed out)
  bool NDIlib_recv_ptz_zoom(
    NDIlib_recv_instance_t p_instance,
    double zoom_value,
  ) {
    return _NDIlib_recv_ptz_zoom(
          p_instance,
          zoom_value,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_zoomPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(
              NDIlib_recv_instance_t, ffi.Float)>>('NDIlib_recv_ptz_zoom');
  late final _NDIlib_recv_ptz_zoom = _NDIlib_recv_ptz_zoomPtr.asFunction<
      int Function(NDIlib_recv_instance_t, double)>();

  /// Zoom at a particular speed
  /// zoom_speed = -1.0 (zoom outwards) ... +1.0 (zoom inwards)
  bool NDIlib_recv_ptz_zoom_speed(
    NDIlib_recv_instance_t p_instance,
    double zoom_speed,
  ) {
    return _NDIlib_recv_ptz_zoom_speed(
          p_instance,
          zoom_speed,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_zoom_speedPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Float)>>('NDIlib_recv_ptz_zoom_speed');
  late final _NDIlib_recv_ptz_zoom_speed = _NDIlib_recv_ptz_zoom_speedPtr
      .asFunction<int Function(NDIlib_recv_instance_t, double)>();

  /// Set the pan and tilt to an absolute value
  /// pan_value  = -1.0 (left) ... 0.0 (centered) ... +1.0 (right)
  /// tilt_value = -1.0 (bottom) ... 0.0 (centered) ... +1.0 (top)
  bool NDIlib_recv_ptz_pan_tilt(
    NDIlib_recv_instance_t p_instance,
    double pan_value,
    double tilt_value,
  ) {
    return _NDIlib_recv_ptz_pan_tilt(
          p_instance,
          pan_value,
          tilt_value,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_pan_tiltPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t, ffi.Float,
              ffi.Float)>>('NDIlib_recv_ptz_pan_tilt');
  late final _NDIlib_recv_ptz_pan_tilt = _NDIlib_recv_ptz_pan_tiltPtr
      .asFunction<int Function(NDIlib_recv_instance_t, double, double)>();

  /// Set the pan and tilt direction and speed
  /// pan_speed = -1.0 (moving right) ... 0.0 (stopped) ... +1.0 (moving left)
  /// tilt_speed = -1.0 (down) ... 0.0 (stopped) ... +1.0 (moving up)
  bool NDIlib_recv_ptz_pan_tilt_speed(
    NDIlib_recv_instance_t p_instance,
    double pan_speed,
    double tilt_speed,
  ) {
    return _NDIlib_recv_ptz_pan_tilt_speed(
          p_instance,
          pan_speed,
          tilt_speed,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_pan_tilt_speedPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t, ffi.Float,
              ffi.Float)>>('NDIlib_recv_ptz_pan_tilt_speed');
  late final _NDIlib_recv_ptz_pan_tilt_speed =
      _NDIlib_recv_ptz_pan_tilt_speedPtr.asFunction<
          int Function(NDIlib_recv_instance_t, double, double)>();

  /// Store the current position, focus, etc... as a preset.
  /// preset_no = 0 ... 99
  bool NDIlib_recv_ptz_store_preset(
    NDIlib_recv_instance_t p_instance,
    int preset_no,
  ) {
    return _NDIlib_recv_ptz_store_preset(
          p_instance,
          preset_no,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_store_presetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Int32)>>('NDIlib_recv_ptz_store_preset');
  late final _NDIlib_recv_ptz_store_preset = _NDIlib_recv_ptz_store_presetPtr
      .asFunction<int Function(NDIlib_recv_instance_t, int)>();

  /// Recall a preset, including position, focus, etc...
  /// preset_no = 0 ... 99
  /// speed = 0.0(as slow as possible) ... 1.0(as fast as possible) The speed at which to move to the new preset
  bool NDIlib_recv_ptz_recall_preset(
    NDIlib_recv_instance_t p_instance,
    int preset_no,
    double speed,
  ) {
    return _NDIlib_recv_ptz_recall_preset(
          p_instance,
          preset_no,
          speed,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_recall_presetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t, ffi.Int32,
              ffi.Float)>>('NDIlib_recv_ptz_recall_preset');
  late final _NDIlib_recv_ptz_recall_preset = _NDIlib_recv_ptz_recall_presetPtr
      .asFunction<int Function(NDIlib_recv_instance_t, int, double)>();

  /// Put the camera in auto-focus
  bool NDIlib_recv_ptz_auto_focus(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_auto_focus(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_auto_focusPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_auto_focus');
  late final _NDIlib_recv_ptz_auto_focus = _NDIlib_recv_ptz_auto_focusPtr
      .asFunction<int Function(NDIlib_recv_instance_t)>();

  /// Focus to an absolute value.
  /// focus_value = 0.0 (focused to infinity) ... 1.0 (focused as close as possible)
  bool NDIlib_recv_ptz_focus(
    NDIlib_recv_instance_t p_instance,
    double focus_value,
  ) {
    return _NDIlib_recv_ptz_focus(
          p_instance,
          focus_value,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_focusPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(
              NDIlib_recv_instance_t, ffi.Float)>>('NDIlib_recv_ptz_focus');
  late final _NDIlib_recv_ptz_focus = _NDIlib_recv_ptz_focusPtr.asFunction<
      int Function(NDIlib_recv_instance_t, double)>();

  /// Focus at a particular speed
  /// focus_speed = -1.0 (focus outwards) ... +1.0 (focus inwards)
  bool NDIlib_recv_ptz_focus_speed(
    NDIlib_recv_instance_t p_instance,
    double focus_speed,
  ) {
    return _NDIlib_recv_ptz_focus_speed(
          p_instance,
          focus_speed,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_focus_speedPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Float)>>('NDIlib_recv_ptz_focus_speed');
  late final _NDIlib_recv_ptz_focus_speed = _NDIlib_recv_ptz_focus_speedPtr
      .asFunction<int Function(NDIlib_recv_instance_t, double)>();

  /// Put the camera in auto white balance mode
  bool NDIlib_recv_ptz_white_balance_auto(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_white_balance_auto(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_white_balance_autoPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_white_balance_auto');
  late final _NDIlib_recv_ptz_white_balance_auto =
      _NDIlib_recv_ptz_white_balance_autoPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Put the camera in indoor white balance
  bool NDIlib_recv_ptz_white_balance_indoor(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_white_balance_indoor(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_white_balance_indoorPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_white_balance_indoor');
  late final _NDIlib_recv_ptz_white_balance_indoor =
      _NDIlib_recv_ptz_white_balance_indoorPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Put the camera in indoor white balance
  bool NDIlib_recv_ptz_white_balance_outdoor(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_white_balance_outdoor(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_white_balance_outdoorPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_white_balance_outdoor');
  late final _NDIlib_recv_ptz_white_balance_outdoor =
      _NDIlib_recv_ptz_white_balance_outdoorPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Use the current brightness to automatically set the current white balance
  bool NDIlib_recv_ptz_white_balance_oneshot(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_white_balance_oneshot(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_white_balance_oneshotPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_white_balance_oneshot');
  late final _NDIlib_recv_ptz_white_balance_oneshot =
      _NDIlib_recv_ptz_white_balance_oneshotPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Set the manual camera white balance using the R, B values
  /// red = 0.0(not red) ... 1.0(very red)
  /// blue = 0.0(not blue) ... 1.0(very blue)
  bool NDIlib_recv_ptz_white_balance_manual(
    NDIlib_recv_instance_t p_instance,
    double red,
    double blue,
  ) {
    return _NDIlib_recv_ptz_white_balance_manual(
          p_instance,
          red,
          blue,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_white_balance_manualPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t, ffi.Float,
              ffi.Float)>>('NDIlib_recv_ptz_white_balance_manual');
  late final _NDIlib_recv_ptz_white_balance_manual =
      _NDIlib_recv_ptz_white_balance_manualPtr.asFunction<
          int Function(NDIlib_recv_instance_t, double, double)>();

  /// Put the camera in auto-exposure mode
  bool NDIlib_recv_ptz_exposure_auto(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_ptz_exposure_auto(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_exposure_autoPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_ptz_exposure_auto');
  late final _NDIlib_recv_ptz_exposure_auto = _NDIlib_recv_ptz_exposure_autoPtr
      .asFunction<int Function(NDIlib_recv_instance_t)>();

  /// Manually set the camera exposure iris
  /// exposure_level = 0.0(dark) ... 1.0(light)
  bool NDIlib_recv_ptz_exposure_manual(
    NDIlib_recv_instance_t p_instance,
    double exposure_level,
  ) {
    return _NDIlib_recv_ptz_exposure_manual(
          p_instance,
          exposure_level,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_exposure_manualPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Float)>>('NDIlib_recv_ptz_exposure_manual');
  late final _NDIlib_recv_ptz_exposure_manual =
      _NDIlib_recv_ptz_exposure_manualPtr.asFunction<
          int Function(NDIlib_recv_instance_t, double)>();

  /// Manually set the camera exposure parameters
  /// iris = 0.0(dark) ... 1.0(light)
  /// gain = 0.0(dark) ... 1.0(light)
  /// shutter_speed = 0.0(slow) ... 1.0(fast)
  bool NDIlib_recv_ptz_exposure_manual_v2(
    NDIlib_recv_instance_t p_instance,
    double iris,
    double gain,
    double shutter_speed,
  ) {
    return _NDIlib_recv_ptz_exposure_manual_v2(
          p_instance,
          iris,
          gain,
          shutter_speed,
        ) !=
        0;
  }

  late final _NDIlib_recv_ptz_exposure_manual_v2Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t, ffi.Float, ffi.Float,
              ffi.Float)>>('NDIlib_recv_ptz_exposure_manual_v2');
  late final _NDIlib_recv_ptz_exposure_manual_v2 =
      _NDIlib_recv_ptz_exposure_manual_v2Ptr.asFunction<
          int Function(NDIlib_recv_instance_t, double, double, double)>();

  /// Recording control
  /// This will start recording.If the recorder was already recording then the message is ignored.A filename is
  /// passed in as a "hint".Since the recorder might already be recording(or might not allow complete
  /// flexibility over its filename), the filename might or might not be used.If the filename is empty, or not
  /// present, a name will be chosen automatically. If you do not with to provide a filename hint you can simply
  /// pass NULL.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  bool NDIlib_recv_recording_start(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<ffi.Int8> p_filename_hint,
  ) {
    return _NDIlib_recv_recording_start(
          p_instance,
          p_filename_hint,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_startPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Pointer<ffi.Int8>)>>('NDIlib_recv_recording_start');
  late final _NDIlib_recv_recording_start =
      _NDIlib_recv_recording_startPtr.asFunction<
          int Function(NDIlib_recv_instance_t, ffi.Pointer<ffi.Int8>)>();

  /// Stop recording.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  bool NDIlib_recv_recording_stop(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_recording_stop(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_stopPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_recording_stop');
  late final _NDIlib_recv_recording_stop = _NDIlib_recv_recording_stopPtr
      .asFunction<int Function(NDIlib_recv_instance_t)>();

  /// This will control the audio level for the recording.dB is specified in decibels relative to the reference
  /// level of the source. Not all recording sources support controlling audio levels.For instance, a digital
  /// audio device would not be able to avoid clipping on sources already at the wrong level, thus might not
  /// support this message.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  bool NDIlib_recv_recording_set_audio_level(
    NDIlib_recv_instance_t p_instance,
    double level_dB,
  ) {
    return _NDIlib_recv_recording_set_audio_level(
          p_instance,
          level_dB,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_set_audio_levelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_recv_instance_t,
              ffi.Float)>>('NDIlib_recv_recording_set_audio_level');
  late final _NDIlib_recv_recording_set_audio_level =
      _NDIlib_recv_recording_set_audio_levelPtr.asFunction<
          int Function(NDIlib_recv_instance_t, double)>();

  /// This will determine if the source is currently recording. It will return true while recording is in
  /// progress and false when it is not. Because there is one recorded and multiple people might be connected to
  /// it, there is a chance that it is recording which was initiated by someone else.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  bool NDIlib_recv_recording_is_recording(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_recording_is_recording(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_is_recordingPtr =
      _lookup<ffi.NativeFunction<ffi.Uint8 Function(NDIlib_recv_instance_t)>>(
          'NDIlib_recv_recording_is_recording');
  late final _NDIlib_recv_recording_is_recording =
      _NDIlib_recv_recording_is_recordingPtr.asFunction<
          int Function(NDIlib_recv_instance_t)>();

  /// Get the current filename for recording. When this is set it will return a non-NULL value which is owned by
  /// you and freed using NDIlib_recv_free_string. If a file was already being recorded by another client, the
  /// massage will contain the name of that file. The filename contains a UNC path (when one is available) to
  /// the recorded file, and can be used to access the file on your local machine for playback.  If a UNC path
  /// is not available, then this will represent the local filename. This will remain valid even after the file
  /// has stopped being recorded until the next file is started.
  ffi.Pointer<ffi.Int8> NDIlib_recv_recording_get_filename(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_recording_get_filename(
      p_instance,
    );
  }

  late final _NDIlib_recv_recording_get_filenamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Int8> Function(
              NDIlib_recv_instance_t)>>('NDIlib_recv_recording_get_filename');
  late final _NDIlib_recv_recording_get_filename =
      _NDIlib_recv_recording_get_filenamePtr.asFunction<
          ffi.Pointer<ffi.Int8> Function(NDIlib_recv_instance_t)>();

  /// This will tell you whether there was a recording error and what that string is. When this is set it will
  /// return a non-NULL value which is owned by you and freed using NDIlib_recv_free_string. When there is no
  /// error it will return NULL.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  ffi.Pointer<ffi.Int8> NDIlib_recv_recording_get_error(
    NDIlib_recv_instance_t p_instance,
  ) {
    return _NDIlib_recv_recording_get_error(
      p_instance,
    );
  }

  late final _NDIlib_recv_recording_get_errorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Int8> Function(
              NDIlib_recv_instance_t)>>('NDIlib_recv_recording_get_error');
  late final _NDIlib_recv_recording_get_error =
      _NDIlib_recv_recording_get_errorPtr.asFunction<
          ffi.Pointer<ffi.Int8> Function(NDIlib_recv_instance_t)>();

  /// Get the current recording times.
  ///
  /// See note above on depreciation and why this is, and how to replace this functionality.
  bool NDIlib_recv_recording_get_times(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_recv_recording_time_t> p_times,
  ) {
    return _NDIlib_recv_recording_get_times(
          p_instance,
          p_times,
        ) !=
        0;
  }

  late final _NDIlib_recv_recording_get_timesPtr = _lookup<
          ffi.NativeFunction<
              ffi.Uint8 Function(NDIlib_recv_instance_t,
                  ffi.Pointer<NDIlib_recv_recording_time_t>)>>(
      'NDIlib_recv_recording_get_times');
  late final _NDIlib_recv_recording_get_times =
      _NDIlib_recv_recording_get_timesPtr.asFunction<
          int Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_recv_recording_time_t>)>();

  /// Create a new sender instance. This will return NULL if it fails. If you specify leave p_create_settings
  /// null then the sender will be created with default settings.
  NDIlib_send_instance_t NDIlib_send_create(
    ffi.Pointer<NDIlib_send_create_t> p_create_settings,
  ) {
    return _NDIlib_send_create(
      p_create_settings,
    );
  }

  late final _NDIlib_send_createPtr = _lookup<
      ffi.NativeFunction<
          NDIlib_send_instance_t Function(
              ffi.Pointer<NDIlib_send_create_t>)>>('NDIlib_send_create');
  late final _NDIlib_send_create = _NDIlib_send_createPtr.asFunction<
      NDIlib_send_instance_t Function(ffi.Pointer<NDIlib_send_create_t>)>();

  /// This will destroy an existing finder instance.
  void NDIlib_send_destroy(
    NDIlib_send_instance_t p_instance,
  ) {
    return _NDIlib_send_destroy(
      p_instance,
    );
  }

  late final _NDIlib_send_destroyPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_send_instance_t)>>(
          'NDIlib_send_destroy');
  late final _NDIlib_send_destroy = _NDIlib_send_destroyPtr.asFunction<
      void Function(NDIlib_send_instance_t)>();

  /// This will add a video frame
  void NDIlib_send_send_video_v2(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
  ) {
    return _NDIlib_send_send_video_v2(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_send_send_video_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_send_send_video_v2');
  late final _NDIlib_send_send_video_v2 =
      _NDIlib_send_send_video_v2Ptr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// This will add a video frame and will return immediately, having scheduled the frame to be displayed. All
  /// processing and sending of the video will occur asynchronously. The memory accessed by NDIlib_video_frame_t
  /// cannot be freed or re-used by the caller until a synchronizing event has occurred. In general the API is
  /// better able to take advantage of asynchronous processing than you might be able to by simple having a
  /// separate thread to submit frames.
  ///
  /// This call is particularly beneficial when processing BGRA video since it allows any color conversion,
  /// compression and network sending to all be done on separate threads from your main rendering thread.
  ///
  /// Synchronizing events are :
  /// - a call to NDIlib_send_send_video
  /// - a call to NDIlib_send_send_video_async with another frame to be sent
  /// - a call to NDIlib_send_send_video with p_video_data=NULL
  /// - a call to NDIlib_send_destroy
  void NDIlib_send_send_video_async_v2(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
  ) {
    return _NDIlib_send_send_video_async_v2(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_send_send_video_async_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_send_send_video_async_v2');
  late final _NDIlib_send_send_video_async_v2 =
      _NDIlib_send_send_video_async_v2Ptr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// This will add an audio frame
  void NDIlib_send_send_audio_v2(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_audio_data,
  ) {
    return _NDIlib_send_send_audio_v2(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_send_send_audio_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_send_send_audio_v2');
  late final _NDIlib_send_send_audio_v2 =
      _NDIlib_send_send_audio_v2Ptr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  /// This will add an audio frame
  void NDIlib_send_send_audio_v3(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v3_t> p_audio_data,
  ) {
    return _NDIlib_send_send_audio_v3(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_send_send_audio_v3Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v3_t>)>>(
      'NDIlib_send_send_audio_v3');
  late final _NDIlib_send_send_audio_v3 =
      _NDIlib_send_send_audio_v3Ptr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_audio_frame_v3_t>)>();

  /// This will add a metadata frame
  void NDIlib_send_send_metadata(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_send_send_metadata(
      p_instance,
      p_metadata,
    );
  }

  late final _NDIlib_send_send_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_send_send_metadata');
  late final _NDIlib_send_send_metadata =
      _NDIlib_send_send_metadataPtr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// This allows you to receive metadata from the other end of the connection
  int NDIlib_send_capture(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
    int timeout_in_ms,
  ) {
    return _NDIlib_send_capture(
      p_instance,
      p_metadata,
      timeout_in_ms,
    );
  }

  late final _NDIlib_send_capturePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_metadata_frame_t>,
              ffi.Uint32)>>('NDIlib_send_capture');
  late final _NDIlib_send_capture = _NDIlib_send_capturePtr.asFunction<
      int Function(
          NDIlib_send_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>, int)>();

  /// Free the buffers returned by capture for metadata
  void NDIlib_send_free_metadata(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_send_free_metadata(
      p_instance,
      p_metadata,
    );
  }

  late final _NDIlib_send_free_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_send_free_metadata');
  late final _NDIlib_send_free_metadata =
      _NDIlib_send_free_metadataPtr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// Determine the current tally sate. If you specify a timeout then it will wait until it has changed,
  /// otherwise it will simply poll it and return the current tally immediately. The return value is whether
  /// anything has actually change (true) or whether it timed out (false)
  bool NDIlib_send_get_tally(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_tally_t> p_tally,
    int timeout_in_ms,
  ) {
    return _NDIlib_send_get_tally(
          p_instance,
          p_tally,
          timeout_in_ms,
        ) !=
        0;
  }

  late final _NDIlib_send_get_tallyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(
              NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_tally_t>,
              ffi.Uint32)>>('NDIlib_send_get_tally');
  late final _NDIlib_send_get_tally = _NDIlib_send_get_tallyPtr.asFunction<
      int Function(NDIlib_send_instance_t, ffi.Pointer<NDIlib_tally_t>, int)>();

  /// Get the current number of receivers connected to this source. This can be used to avoid even rendering
  /// when nothing is connected to the video source. which can significantly improve the efficiency if you want
  /// to make a lot of sources available on the network. If you specify a timeout that is not 0 then it will
  /// wait until there are connections for this amount of time.
  int NDIlib_send_get_no_connections(
    NDIlib_send_instance_t p_instance,
    int timeout_in_ms,
  ) {
    return _NDIlib_send_get_no_connections(
      p_instance,
      timeout_in_ms,
    );
  }

  late final _NDIlib_send_get_no_connectionsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(NDIlib_send_instance_t,
              ffi.Uint32)>>('NDIlib_send_get_no_connections');
  late final _NDIlib_send_get_no_connections =
      _NDIlib_send_get_no_connectionsPtr.asFunction<
          int Function(NDIlib_send_instance_t, int)>();

  /// Connection based metadata is data that is sent automatically each time a new connection is received. You
  /// queue all of these up and they are sent on each connection. To reset them you need to clear them all and
  /// set them up again.
  void NDIlib_send_clear_connection_metadata(
    NDIlib_send_instance_t p_instance,
  ) {
    return _NDIlib_send_clear_connection_metadata(
      p_instance,
    );
  }

  late final _NDIlib_send_clear_connection_metadataPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_send_instance_t)>>(
          'NDIlib_send_clear_connection_metadata');
  late final _NDIlib_send_clear_connection_metadata =
      _NDIlib_send_clear_connection_metadataPtr.asFunction<
          void Function(NDIlib_send_instance_t)>();

  /// Add a connection metadata string to the list of what is sent on each new connection. If someone is already
  /// connected then this string will be sent to them immediately.
  void NDIlib_send_add_connection_metadata(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
  ) {
    return _NDIlib_send_add_connection_metadata(
      p_instance,
      p_metadata,
    );
  }

  late final _NDIlib_send_add_connection_metadataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_metadata_frame_t>)>>(
      'NDIlib_send_add_connection_metadata');
  late final _NDIlib_send_add_connection_metadata =
      _NDIlib_send_add_connection_metadataPtr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_metadata_frame_t>)>();

  /// This will assign a new fail-over source for this video source. What this means is that if this video
  /// source was to fail any receivers would automatically switch over to use this source, unless this source
  /// then came back online. You can specify NULL to clear the source.
  void NDIlib_send_set_failover(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_source_t> p_failover_source,
  ) {
    return _NDIlib_send_set_failover(
      p_instance,
      p_failover_source,
    );
  }

  late final _NDIlib_send_set_failoverPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_source_t>)>>('NDIlib_send_set_failover');
  late final _NDIlib_send_set_failover =
      _NDIlib_send_set_failoverPtr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_source_t>)>();

  /// Retrieve the source information for the given sender instance.  This pointer is valid until NDIlib_send_destroy is called.
  ffi.Pointer<NDIlib_source_t> NDIlib_send_get_source_name(
    NDIlib_send_instance_t p_instance,
  ) {
    return _NDIlib_send_get_source_name(
      p_instance,
    );
  }

  late final _NDIlib_send_get_source_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<NDIlib_source_t> Function(
              NDIlib_send_instance_t)>>('NDIlib_send_get_source_name');
  late final _NDIlib_send_get_source_name =
      _NDIlib_send_get_source_namePtr.asFunction<
          ffi.Pointer<NDIlib_source_t> Function(NDIlib_send_instance_t)>();

  /// Create an NDI routing source
  NDIlib_routing_instance_t NDIlib_routing_create(
    ffi.Pointer<NDIlib_routing_create_t> p_create_settings,
  ) {
    return _NDIlib_routing_create(
      p_create_settings,
    );
  }

  late final _NDIlib_routing_createPtr = _lookup<
      ffi.NativeFunction<
          NDIlib_routing_instance_t Function(
              ffi.Pointer<NDIlib_routing_create_t>)>>('NDIlib_routing_create');
  late final _NDIlib_routing_create = _NDIlib_routing_createPtr.asFunction<
      NDIlib_routing_instance_t Function(
          ffi.Pointer<NDIlib_routing_create_t>)>();

  /// Destroy and NDI routing source
  void NDIlib_routing_destroy(
    NDIlib_routing_instance_t p_instance,
  ) {
    return _NDIlib_routing_destroy(
      p_instance,
    );
  }

  late final _NDIlib_routing_destroyPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(NDIlib_routing_instance_t)>>(
          'NDIlib_routing_destroy');
  late final _NDIlib_routing_destroy = _NDIlib_routing_destroyPtr.asFunction<
      void Function(NDIlib_routing_instance_t)>();

  /// Change the routing of this source to another destination
  bool NDIlib_routing_change(
    NDIlib_routing_instance_t p_instance,
    ffi.Pointer<NDIlib_source_t> p_source,
  ) {
    return _NDIlib_routing_change(
          p_instance,
          p_source,
        ) !=
        0;
  }

  late final _NDIlib_routing_changePtr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(NDIlib_routing_instance_t,
              ffi.Pointer<NDIlib_source_t>)>>('NDIlib_routing_change');
  late final _NDIlib_routing_change = _NDIlib_routing_changePtr.asFunction<
      int Function(NDIlib_routing_instance_t, ffi.Pointer<NDIlib_source_t>)>();

  /// Change the routing of this source to another destination
  bool NDIlib_routing_clear(
    NDIlib_routing_instance_t p_instance,
  ) {
    return _NDIlib_routing_clear(
          p_instance,
        ) !=
        0;
  }

  late final _NDIlib_routing_clearPtr = _lookup<
          ffi.NativeFunction<ffi.Uint8 Function(NDIlib_routing_instance_t)>>(
      'NDIlib_routing_clear');
  late final _NDIlib_routing_clear = _NDIlib_routing_clearPtr.asFunction<
      int Function(NDIlib_routing_instance_t)>();

  /// Get the current number of receivers connected to this source. This can be used to avoid even rendering
  /// when nothing is connected to the video source. which can significantly improve the efficiency if you want
  /// to make a lot of sources available on the network. If you specify a timeout that is not 0 then it will
  /// wait until there are connections for this amount of time.
  int NDIlib_routing_get_no_connections(
    NDIlib_routing_instance_t p_instance,
    int timeout_in_ms,
  ) {
    return _NDIlib_routing_get_no_connections(
      p_instance,
      timeout_in_ms,
    );
  }

  late final _NDIlib_routing_get_no_connectionsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(NDIlib_routing_instance_t,
              ffi.Uint32)>>('NDIlib_routing_get_no_connections');
  late final _NDIlib_routing_get_no_connections =
      _NDIlib_routing_get_no_connectionsPtr.asFunction<
          int Function(NDIlib_routing_instance_t, int)>();

  /// Retrieve the source information for the given router instance.  This pointer is valid until
  /// NDIlib_routing_destroy is called.
  ffi.Pointer<NDIlib_source_t> NDIlib_routing_get_source_name(
    NDIlib_routing_instance_t p_instance,
  ) {
    return _NDIlib_routing_get_source_name(
      p_instance,
    );
  }

  late final _NDIlib_routing_get_source_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<NDIlib_source_t> Function(
              NDIlib_routing_instance_t)>>('NDIlib_routing_get_source_name');
  late final _NDIlib_routing_get_source_name =
      _NDIlib_routing_get_source_namePtr.asFunction<
          ffi.Pointer<NDIlib_source_t> Function(NDIlib_routing_instance_t)>();

  /// This will add an audio frame in interleaved 16bpp.
  void NDIlib_util_send_send_audio_interleaved_16s(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t> p_audio_data,
  ) {
    return _NDIlib_util_send_send_audio_interleaved_16s(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_util_send_send_audio_interleaved_16sPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>>(
      'NDIlib_util_send_send_audio_interleaved_16s');
  late final _NDIlib_util_send_send_audio_interleaved_16s =
      _NDIlib_util_send_send_audio_interleaved_16sPtr.asFunction<
          void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>();

  /// This will add an audio frame in interleaved 32bpp.
  void NDIlib_util_send_send_audio_interleaved_32s(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t> p_audio_data,
  ) {
    return _NDIlib_util_send_send_audio_interleaved_32s(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_util_send_send_audio_interleaved_32sPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>)>>(
      'NDIlib_util_send_send_audio_interleaved_32s');
  late final _NDIlib_util_send_send_audio_interleaved_32s =
      _NDIlib_util_send_send_audio_interleaved_32sPtr.asFunction<
          void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>)>();

  /// This will add an audio frame in interleaved floating point.
  void NDIlib_util_send_send_audio_interleaved_32f(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t> p_audio_data,
  ) {
    return _NDIlib_util_send_send_audio_interleaved_32f(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_util_send_send_audio_interleaved_32fPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_send_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>>(
      'NDIlib_util_send_send_audio_interleaved_32f');
  late final _NDIlib_util_send_send_audio_interleaved_32f =
      _NDIlib_util_send_send_audio_interleaved_32fPtr.asFunction<
          void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>();

  /// Convert to interleaved 16bpp.
  void NDIlib_util_audio_to_interleaved_16s_v2(
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t> p_dst,
  ) {
    return _NDIlib_util_audio_to_interleaved_16s_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_to_interleaved_16s_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>>(
      'NDIlib_util_audio_to_interleaved_16s_v2');
  late final _NDIlib_util_audio_to_interleaved_16s_v2 =
      _NDIlib_util_audio_to_interleaved_16s_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
              ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>();

  /// Convert from interleaved 16bpp.
  void NDIlib_util_audio_from_interleaved_16s_v2(
    ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_dst,
  ) {
    return _NDIlib_util_audio_from_interleaved_16s_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_from_interleaved_16s_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_util_audio_from_interleaved_16s_v2');
  late final _NDIlib_util_audio_from_interleaved_16s_v2 =
      _NDIlib_util_audio_from_interleaved_16s_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>,
              ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  /// Convert to interleaved 32bpp.
  void NDIlib_util_audio_to_interleaved_32s_v2(
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t> p_dst,
  ) {
    return _NDIlib_util_audio_to_interleaved_32s_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_to_interleaved_32s_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>)>>(
      'NDIlib_util_audio_to_interleaved_32s_v2');
  late final _NDIlib_util_audio_to_interleaved_32s_v2 =
      _NDIlib_util_audio_to_interleaved_32s_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
              ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>)>();

  /// Convert from interleaved 32bpp.
  void NDIlib_util_audio_from_interleaved_32s_v2(
    ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_dst,
  ) {
    return _NDIlib_util_audio_from_interleaved_32s_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_from_interleaved_32s_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_util_audio_from_interleaved_32s_v2');
  late final _NDIlib_util_audio_from_interleaved_32s_v2 =
      _NDIlib_util_audio_from_interleaved_32s_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_interleaved_32s_t>,
              ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  /// Convert to interleaved floating point.
  void NDIlib_util_audio_to_interleaved_32f_v2(
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t> p_dst,
  ) {
    return _NDIlib_util_audio_to_interleaved_32f_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_to_interleaved_32f_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>>(
      'NDIlib_util_audio_to_interleaved_32f_v2');
  late final _NDIlib_util_audio_to_interleaved_32f_v2 =
      _NDIlib_util_audio_to_interleaved_32f_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_v2_t>,
              ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>();

  /// Convert from interleaved floating point.
  void NDIlib_util_audio_from_interleaved_32f_v2(
    ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_dst,
  ) {
    return _NDIlib_util_audio_from_interleaved_32f_v2(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_from_interleaved_32f_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_util_audio_from_interleaved_32f_v2');
  late final _NDIlib_util_audio_from_interleaved_32f_v2 =
      _NDIlib_util_audio_from_interleaved_32f_v2Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>,
              ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  /// This is a helper function that you may use to convert from 10bit packed UYVY into 16bit semi-planar. The
  /// FourCC on the source is ignored in this function since we do not define a V210 format in NDI. You must
  /// make sure that there is memory and a stride allocated in p_dst.
  void NDIlib_util_V210_to_P216(
    ffi.Pointer<NDIlib_video_frame_v2_t> p_src_v210,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_dst_p216,
  ) {
    return _NDIlib_util_V210_to_P216(
      p_src_v210,
      p_dst_p216,
    );
  }

  late final _NDIlib_util_V210_to_P216Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_video_frame_v2_t>,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_util_V210_to_P216');
  late final _NDIlib_util_V210_to_P216 =
      _NDIlib_util_V210_to_P216Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_video_frame_v2_t>,
              ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// This converts from 16bit semi-planar to 10bit. You must make sure that there is memory and a stride
  /// allocated in p_dst.
  void NDIlib_util_P216_to_V210(
    ffi.Pointer<NDIlib_video_frame_v2_t> p_src_p216,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_dst_v210,
  ) {
    return _NDIlib_util_P216_to_V210(
      p_src_p216,
      p_dst_v210,
    );
  }

  late final _NDIlib_util_P216_to_V210Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_video_frame_v2_t>,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_util_P216_to_V210');
  late final _NDIlib_util_P216_to_V210 =
      _NDIlib_util_P216_to_V210Ptr.asFunction<
          void Function(ffi.Pointer<NDIlib_video_frame_v2_t>,
              ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// For legacy reasons I called this the wrong thing. For backwards compatibility.
  NDIlib_find_instance_t NDIlib_find_create2(
    ffi.Pointer<NDIlib_find_create_t> p_create_settings,
  ) {
    return _NDIlib_find_create2(
      p_create_settings,
    );
  }

  late final _NDIlib_find_create2Ptr = _lookup<
      ffi.NativeFunction<
          NDIlib_find_instance_t Function(
              ffi.Pointer<NDIlib_find_create_t>)>>('NDIlib_find_create2');
  late final _NDIlib_find_create2 = _NDIlib_find_create2Ptr.asFunction<
      NDIlib_find_instance_t Function(ffi.Pointer<NDIlib_find_create_t>)>();

  NDIlib_find_instance_t NDIlib_find_create(
    ffi.Pointer<NDIlib_find_create_t> p_create_settings,
  ) {
    return _NDIlib_find_create(
      p_create_settings,
    );
  }

  late final _NDIlib_find_createPtr = _lookup<
      ffi.NativeFunction<
          NDIlib_find_instance_t Function(
              ffi.Pointer<NDIlib_find_create_t>)>>('NDIlib_find_create');
  late final _NDIlib_find_create = _NDIlib_find_createPtr.asFunction<
      NDIlib_find_instance_t Function(ffi.Pointer<NDIlib_find_create_t>)>();

  /// DEPRECATED. This function is basically exactly the following and was confusing to use.
  /// if ((!timeout_in_ms) || (NDIlib_find_wait_for_sources(timeout_in_ms)))
  /// return NDIlib_find_get_current_sources(p_instance, p_no_sources);
  /// return NULL;
  ffi.Pointer<NDIlib_source_t> NDIlib_find_get_sources(
    NDIlib_find_instance_t p_instance,
    ffi.Pointer<ffi.Uint32> p_no_sources,
    int timeout_in_ms,
  ) {
    return _NDIlib_find_get_sources(
      p_instance,
      p_no_sources,
      timeout_in_ms,
    );
  }

  late final _NDIlib_find_get_sourcesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<NDIlib_source_t> Function(NDIlib_find_instance_t,
              ffi.Pointer<ffi.Uint32>, ffi.Uint32)>>('NDIlib_find_get_sources');
  late final _NDIlib_find_get_sources = _NDIlib_find_get_sourcesPtr.asFunction<
      ffi.Pointer<NDIlib_source_t> Function(
          NDIlib_find_instance_t, ffi.Pointer<ffi.Uint32>, int)>();

  /// This function is deprecated, please use NDIlib_recv_create_v3 if you can. Using this function will
  /// continue to work, and be supported for backwards compatibility. If the input parameter is NULL it will be
  /// created with default settings and an automatically determined receiver name.
  NDIlib_recv_instance_t NDIlib_recv_create_v2(
    ffi.Pointer<NDIlib_recv_create_t> p_create_settings,
  ) {
    return _NDIlib_recv_create_v2(
      p_create_settings,
    );
  }

  late final _NDIlib_recv_create_v2Ptr = _lookup<
      ffi.NativeFunction<
          NDIlib_recv_instance_t Function(
              ffi.Pointer<NDIlib_recv_create_t>)>>('NDIlib_recv_create_v2');
  late final _NDIlib_recv_create_v2 = _NDIlib_recv_create_v2Ptr.asFunction<
      NDIlib_recv_instance_t Function(ffi.Pointer<NDIlib_recv_create_t>)>();

  /// For legacy reasons I called this the wrong thing. For backwards compatibility. If the input parameter is
  /// NULL it will be created with default settings and an automatically determined receiver name.
  NDIlib_recv_instance_t NDIlib_recv_create2(
    ffi.Pointer<NDIlib_recv_create_t> p_create_settings,
  ) {
    return _NDIlib_recv_create2(
      p_create_settings,
    );
  }

  late final _NDIlib_recv_create2Ptr = _lookup<
      ffi.NativeFunction<
          NDIlib_recv_instance_t Function(
              ffi.Pointer<NDIlib_recv_create_t>)>>('NDIlib_recv_create2');
  late final _NDIlib_recv_create2 = _NDIlib_recv_create2Ptr.asFunction<
      NDIlib_recv_instance_t Function(ffi.Pointer<NDIlib_recv_create_t>)>();

  /// This function is deprecated, please use NDIlib_recv_create_v3 if you can. Using this function will
  /// continue to work, and be supported for backwards compatibility. This version sets bandwidth to highest and
  /// allow fields to true. If the input parameter is NULL it will be created with default settings and an
  /// automatically determined receiver name.
  NDIlib_recv_instance_t NDIlib_recv_create(
    ffi.Pointer<NDIlib_recv_create_t> p_create_settings,
  ) {
    return _NDIlib_recv_create(
      p_create_settings,
    );
  }

  late final _NDIlib_recv_createPtr = _lookup<
      ffi.NativeFunction<
          NDIlib_recv_instance_t Function(
              ffi.Pointer<NDIlib_recv_create_t>)>>('NDIlib_recv_create');
  late final _NDIlib_recv_create = _NDIlib_recv_createPtr.asFunction<
      NDIlib_recv_instance_t Function(ffi.Pointer<NDIlib_recv_create_t>)>();

  /// This will allow you to receive video, audio and metadata frames. Any of the buffers can be NULL, in which
  /// case data of that type will not be captured in this call. This call can be called simultaneously on
  /// separate threads, so it is entirely possible to receive audio, video, metadata all on separate threads.
  /// This function will return NDIlib_frame_type_none if no data is received within the specified timeout and
  /// NDIlib_frame_type_error if the connection is lost. Buffers captured with this must be freed with the
  /// appropriate free function below.
  int NDIlib_recv_capture(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_t> p_video_data,
    ffi.Pointer<NDIlib_audio_frame_t> p_audio_data,
    ffi.Pointer<NDIlib_metadata_frame_t> p_metadata,
    int timeout_in_ms,
  ) {
    return _NDIlib_recv_capture(
      p_instance,
      p_video_data,
      p_audio_data,
      p_metadata,
      timeout_in_ms,
    );
  }

  late final _NDIlib_recv_capturePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_video_frame_t>,
              ffi.Pointer<NDIlib_audio_frame_t>,
              ffi.Pointer<NDIlib_metadata_frame_t>,
              ffi.Uint32)>>('NDIlib_recv_capture');
  late final _NDIlib_recv_capture = _NDIlib_recv_capturePtr.asFunction<
      int Function(
          NDIlib_recv_instance_t,
          ffi.Pointer<NDIlib_video_frame_t>,
          ffi.Pointer<NDIlib_audio_frame_t>,
          ffi.Pointer<NDIlib_metadata_frame_t>,
          int)>();

  /// Free the buffers returned by capture for video
  void NDIlib_recv_free_video(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_t> p_video_data,
  ) {
    return _NDIlib_recv_free_video(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_recv_free_videoPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_video_frame_t>)>>('NDIlib_recv_free_video');
  late final _NDIlib_recv_free_video = _NDIlib_recv_free_videoPtr.asFunction<
      void Function(
          NDIlib_recv_instance_t, ffi.Pointer<NDIlib_video_frame_t>)>();

  /// Free the buffers returned by capture for audio
  void NDIlib_recv_free_audio(
    NDIlib_recv_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_t> p_audio_data,
  ) {
    return _NDIlib_recv_free_audio(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_recv_free_audioPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_recv_instance_t,
              ffi.Pointer<NDIlib_audio_frame_t>)>>('NDIlib_recv_free_audio');
  late final _NDIlib_recv_free_audio = _NDIlib_recv_free_audioPtr.asFunction<
      void Function(
          NDIlib_recv_instance_t, ffi.Pointer<NDIlib_audio_frame_t>)>();

  /// This will add a video frame
  void NDIlib_send_send_video(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_t> p_video_data,
  ) {
    return _NDIlib_send_send_video(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_send_send_videoPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_video_frame_t>)>>('NDIlib_send_send_video');
  late final _NDIlib_send_send_video = _NDIlib_send_send_videoPtr.asFunction<
      void Function(
          NDIlib_send_instance_t, ffi.Pointer<NDIlib_video_frame_t>)>();

  /// This will add a video frame and will return immediately, having scheduled the frame to be displayed. All
  /// processing and sending of the video will occur asynchronously. The memory accessed by NDIlib_video_frame_t
  /// cannot be freed or re-used by the caller until a synchronizing event has occurred. In general the API is
  /// better able to take advantage of asynchronous processing than you might be able to by simple having a
  /// separate thread to submit frames.
  ///
  /// This call is particularly beneficial when processing BGRA video since it allows any color conversion,
  /// compression and network sending to all be done on separate threads from your main rendering thread.
  ///
  /// Synchronizing events are :
  /// - a call to NDIlib_send_send_video
  /// - a call to NDIlib_send_send_video_async with another frame to be sent
  /// - a call to NDIlib_send_send_video with p_video_data=NULL
  /// - a call to NDIlib_send_destroy
  void NDIlib_send_send_video_async(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_t> p_video_data,
  ) {
    return _NDIlib_send_send_video_async(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_send_send_video_asyncPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  NDIlib_send_instance_t, ffi.Pointer<NDIlib_video_frame_t>)>>(
      'NDIlib_send_send_video_async');
  late final _NDIlib_send_send_video_async =
      _NDIlib_send_send_video_asyncPtr.asFunction<
          void Function(
              NDIlib_send_instance_t, ffi.Pointer<NDIlib_video_frame_t>)>();

  /// This will add an audio frame
  void NDIlib_send_send_audio(
    NDIlib_send_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_t> p_audio_data,
  ) {
    return _NDIlib_send_send_audio(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_send_send_audioPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(NDIlib_send_instance_t,
              ffi.Pointer<NDIlib_audio_frame_t>)>>('NDIlib_send_send_audio');
  late final _NDIlib_send_send_audio = _NDIlib_send_send_audioPtr.asFunction<
      void Function(
          NDIlib_send_instance_t, ffi.Pointer<NDIlib_audio_frame_t>)>();

  /// Convert an planar floating point audio buffer into a interleaved short audio buffer.
  /// IMPORTANT : You must allocate the space for the samples in the destination to allow for your own memory management.
  void NDIlib_util_audio_to_interleaved_16s(
    ffi.Pointer<NDIlib_audio_frame_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t> p_dst,
  ) {
    return _NDIlib_util_audio_to_interleaved_16s(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_to_interleaved_16sPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_audio_frame_t>,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>>(
      'NDIlib_util_audio_to_interleaved_16s');
  late final _NDIlib_util_audio_to_interleaved_16s =
      _NDIlib_util_audio_to_interleaved_16sPtr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_t>,
              ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>)>();

  /// Convert an interleaved short audio buffer audio buffer into a planar floating point one.
  /// IMPORTANT : You must allocate the space for the samples in the destination to allow for your own memory management.
  void NDIlib_util_audio_from_interleaved_16s(
    ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_t> p_dst,
  ) {
    return _NDIlib_util_audio_from_interleaved_16s(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_from_interleaved_16sPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>,
                  ffi.Pointer<NDIlib_audio_frame_t>)>>(
      'NDIlib_util_audio_from_interleaved_16s');
  late final _NDIlib_util_audio_from_interleaved_16s =
      _NDIlib_util_audio_from_interleaved_16sPtr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_interleaved_16s_t>,
              ffi.Pointer<NDIlib_audio_frame_t>)>();

  /// Convert an planar floating point audio buffer into a interleaved floating point audio buffer.
  /// IMPORTANT : You must allocate the space for the samples in the destination to allow for your own memory management.
  void NDIlib_util_audio_to_interleaved_32f(
    ffi.Pointer<NDIlib_audio_frame_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t> p_dst,
  ) {
    return _NDIlib_util_audio_to_interleaved_32f(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_to_interleaved_32fPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<NDIlib_audio_frame_t>,
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>>(
      'NDIlib_util_audio_to_interleaved_32f');
  late final _NDIlib_util_audio_to_interleaved_32f =
      _NDIlib_util_audio_to_interleaved_32fPtr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_t>,
              ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>)>();

  /// Convert an interleaved floating point audio buffer into a planar floating point one.
  /// IMPORTANT : You must allocate the space for the samples in the destination to allow for your own memory management.
  void NDIlib_util_audio_from_interleaved_32f(
    ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t> p_src,
    ffi.Pointer<NDIlib_audio_frame_t> p_dst,
  ) {
    return _NDIlib_util_audio_from_interleaved_32f(
      p_src,
      p_dst,
    );
  }

  late final _NDIlib_util_audio_from_interleaved_32fPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>,
                  ffi.Pointer<NDIlib_audio_frame_t>)>>(
      'NDIlib_util_audio_from_interleaved_32f');
  late final _NDIlib_util_audio_from_interleaved_32f =
      _NDIlib_util_audio_from_interleaved_32fPtr.asFunction<
          void Function(ffi.Pointer<NDIlib_audio_frame_interleaved_32f_t>,
              ffi.Pointer<NDIlib_audio_frame_t>)>();

  /// Create a frame synchronizer instance that can be used to get frames from a receiver. Once this receiver
  /// has been bound to a frame-sync then you should use it in order to receive video frames. You can continue
  /// to use the underlying receiver for other operations (tally, PTZ, etc...). Note that it remains your
  /// responsibility to destroy the receiver even when a frame-sync is using it. You should always destroy the
  /// receiver after the frame-sync has been destroyed.
  NDIlib_framesync_instance_t NDIlib_framesync_create(
    NDIlib_recv_instance_t p_receiver,
  ) {
    return _NDIlib_framesync_create(
      p_receiver,
    );
  }

  late final _NDIlib_framesync_createPtr = _lookup<
      ffi.NativeFunction<
          NDIlib_framesync_instance_t Function(
              NDIlib_recv_instance_t)>>('NDIlib_framesync_create');
  late final _NDIlib_framesync_create = _NDIlib_framesync_createPtr.asFunction<
      NDIlib_framesync_instance_t Function(NDIlib_recv_instance_t)>();

  /// Destroy a frame-sync implementation
  void NDIlib_framesync_destroy(
    NDIlib_framesync_instance_t p_instance,
  ) {
    return _NDIlib_framesync_destroy(
      p_instance,
    );
  }

  late final _NDIlib_framesync_destroyPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(NDIlib_framesync_instance_t)>>(
      'NDIlib_framesync_destroy');
  late final _NDIlib_framesync_destroy = _NDIlib_framesync_destroyPtr
      .asFunction<void Function(NDIlib_framesync_instance_t)>();

  /// This function will pull audio samples from the frame-sync queue. This function will always return data
  /// immediately, inserting silence if no current audio data is present. You should call this at the rate that
  /// you want audio and it will automatically adapt the incoming audio signal to match the rate at which you
  /// are calling by using dynamic audio sampling. Note that you have no obligation that your requested sample
  /// rate, no channels and no samples match the incoming signal and all combinations of conversions
  /// are supported.
  ///
  /// If you wish to know what the current incoming audio format is, then you can make a call with the
  /// parameters set to zero and it will then return the associated settings. For instance a call as follows:
  ///
  /// NDIlib_framesync_capture_audio(p_instance, p_audio_data, 0, 0, 0);
  ///
  /// will return in p_audio_data the current received audio format if there is one or sample-rate and
  /// no_channels equal to zero if there is not one. At any time you can specify sample_rate and no_channels as
  /// zero and it will return the current received audio format.
  void NDIlib_framesync_capture_audio(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_audio_data,
    int sample_rate,
    int no_channels,
    int no_samples,
  ) {
    return _NDIlib_framesync_capture_audio(
      p_instance,
      p_audio_data,
      sample_rate,
      no_channels,
      no_samples,
    );
  }

  late final _NDIlib_framesync_capture_audioPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v2_t>,
              ffi.Int32,
              ffi.Int32,
              ffi.Int32)>>('NDIlib_framesync_capture_audio');
  late final _NDIlib_framesync_capture_audio =
      _NDIlib_framesync_capture_audioPtr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v2_t>, int, int, int)>();

  void NDIlib_framesync_capture_audio_v2(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v3_t> p_audio_data,
    int sample_rate,
    int no_channels,
    int no_samples,
  ) {
    return _NDIlib_framesync_capture_audio_v2(
      p_instance,
      p_audio_data,
      sample_rate,
      no_channels,
      no_samples,
    );
  }

  late final _NDIlib_framesync_capture_audio_v2Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v3_t>,
              ffi.Int32,
              ffi.Int32,
              ffi.Int32)>>('NDIlib_framesync_capture_audio_v2');
  late final _NDIlib_framesync_capture_audio_v2 =
      _NDIlib_framesync_capture_audio_v2Ptr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v3_t>, int, int, int)>();

  /// Free audio returned by NDIlib_framesync_capture_audio
  void NDIlib_framesync_free_audio(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v2_t> p_audio_data,
  ) {
    return _NDIlib_framesync_free_audio(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_framesync_free_audioPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_framesync_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v2_t>)>>(
      'NDIlib_framesync_free_audio');
  late final _NDIlib_framesync_free_audio =
      _NDIlib_framesync_free_audioPtr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v2_t>)>();

  void NDIlib_framesync_free_audio_v2(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_audio_frame_v3_t> p_audio_data,
  ) {
    return _NDIlib_framesync_free_audio_v2(
      p_instance,
      p_audio_data,
    );
  }

  late final _NDIlib_framesync_free_audio_v2Ptr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_framesync_instance_t,
                  ffi.Pointer<NDIlib_audio_frame_v3_t>)>>(
      'NDIlib_framesync_free_audio_v2');
  late final _NDIlib_framesync_free_audio_v2 =
      _NDIlib_framesync_free_audio_v2Ptr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_audio_frame_v3_t>)>();

  /// This function will tell you the approximate current depth of the audio queue to give you an indication
  /// of the number of audio samples you can request. Note that if you should treat the results of this function
  /// with some care because in reality the frame-sync API is meant to dynamically resample audio to match the
  /// rate that you are calling it. If you have an inaccurate clock then this function can be useful.
  /// for instance :
  ///
  /// while(true)
  /// {   int no_samples = NDIlib_framesync_audio_queue_depth(p_instance);
  /// NDIlib_framesync_capture_audio( ... );
  /// play_audio( ... )
  /// NDIlib_framesync_free_audio( ... )
  /// inaccurate_sleep( 33ms );
  /// }
  ///
  /// Obviously because audio is being received in real-time there is no guarantee after the call that the
  /// number is correct since new samples might have been captured in that time. On synchronous use of this
  /// function however this will be the minimum number of samples in the queue at any later time until
  /// NDIlib_framesync_capture_audio is called.
  int NDIlib_framesync_audio_queue_depth(
    NDIlib_framesync_instance_t p_instance,
  ) {
    return _NDIlib_framesync_audio_queue_depth(
      p_instance,
    );
  }

  late final _NDIlib_framesync_audio_queue_depthPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(NDIlib_framesync_instance_t)>>(
      'NDIlib_framesync_audio_queue_depth');
  late final _NDIlib_framesync_audio_queue_depth =
      _NDIlib_framesync_audio_queue_depthPtr.asFunction<
          int Function(NDIlib_framesync_instance_t)>();

  /// This function will pull video samples from the frame-sync queue. This function will always immediately
  /// return a video sample by using time-base correction. You can specify the desired field type which is then
  /// used to return the best possible frame. Note that field based frame-synchronization means that the
  /// frame-synchronizer attempts to match the fielded input phase with the frame requests so that you have the
  /// most correct possible field ordering on output. Note that the same frame can be returned multiple times.
  ///
  /// If no video frame has ever been received, this will return NDIlib_video_frame_v2_t as an empty (all zero)
  /// structure. The reason for this is that it allows you to determine that there has not yet been any video
  /// and act accordingly. For instance you might want to display a constant frame output at a particular video
  /// format, or black.
  void NDIlib_framesync_capture_video(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
    int field_type,
  ) {
    return _NDIlib_framesync_capture_video(
      p_instance,
      p_video_data,
      field_type,
    );
  }

  late final _NDIlib_framesync_capture_videoPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_video_frame_v2_t>,
              ffi.Int32)>>('NDIlib_framesync_capture_video');
  late final _NDIlib_framesync_capture_video =
      _NDIlib_framesync_capture_videoPtr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_video_frame_v2_t>, int)>();

  /// Free audio returned by NDIlib_framesync_capture_video
  void NDIlib_framesync_free_video(
    NDIlib_framesync_instance_t p_instance,
    ffi.Pointer<NDIlib_video_frame_v2_t> p_video_data,
  ) {
    return _NDIlib_framesync_free_video(
      p_instance,
      p_video_data,
    );
  }

  late final _NDIlib_framesync_free_videoPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(NDIlib_framesync_instance_t,
                  ffi.Pointer<NDIlib_video_frame_v2_t>)>>(
      'NDIlib_framesync_free_video');
  late final _NDIlib_framesync_free_video =
      _NDIlib_framesync_free_videoPtr.asFunction<
          void Function(NDIlib_framesync_instance_t,
              ffi.Pointer<NDIlib_video_frame_v2_t>)>();

  /// Load the library
  ffi.Pointer<NDIlib_v5> NDIlib_v5_load() {
    return _NDIlib_v5_load();
  }

  late final _NDIlib_v5_loadPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<NDIlib_v5> Function()>>(
          'NDIlib_v5_load');
  late final _NDIlib_v5_load =
      _NDIlib_v5_loadPtr.asFunction<ffi.Pointer<NDIlib_v5> Function()>();

  ffi.Pointer<NDIlib_v4_5> NDIlib_v4_5_load() {
    return _NDIlib_v4_5_load();
  }

  late final _NDIlib_v4_5_loadPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<NDIlib_v4_5> Function()>>(
          'NDIlib_v4_5_load');
  late final _NDIlib_v4_5_load =
      _NDIlib_v4_5_loadPtr.asFunction<ffi.Pointer<NDIlib_v4_5> Function()>();

  /// Load the library
  ffi.Pointer<NDIlib_v4> NDIlib_v4_load() {
    return _NDIlib_v4_load();
  }

  late final _NDIlib_v4_loadPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<NDIlib_v4> Function()>>(
          'NDIlib_v4_load');
  late final _NDIlib_v4_load =
      _NDIlib_v4_loadPtr.asFunction<ffi.Pointer<NDIlib_v4> Function()>();

  /// Load the library
  ffi.Pointer<NDIlib_v3> NDIlib_v3_load() {
    return _NDIlib_v3_load();
  }

  late final _NDIlib_v3_loadPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<NDIlib_v3> Function()>>(
          'NDIlib_v3_load');
  late final _NDIlib_v3_load =
      _NDIlib_v3_loadPtr.asFunction<ffi.Pointer<NDIlib_v3> Function()>();

  ffi.Pointer<NDIlib_v2> NDIlib_v2_load() {
    return _NDIlib_v2_load();
  }

  late final _NDIlib_v2_loadPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<NDIlib_v2> Function()>>(
          'NDIlib_v2_load');
  late final _NDIlib_v2_load =
      _NDIlib_v2_loadPtr.asFunction<ffi.Pointer<NDIlib_v2> Function()>();
}

/// mbstate_t is an opaque object to keep conversion state, during multibyte
/// stream conversions.  The content must not be referenced by user programs.
class __mbstate_t extends ffi.Union {
  @ffi.Array.multi([128])
  external ffi.Array<ffi.Int8> __mbstate8;

  /// for alignment
  @ffi.Int64()
  external int _mbstateL;
}

class __darwin_pthread_handler_rec extends ffi.Struct {
  /// Routine to call
  external ffi
          .Pointer<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ffi.Void>)>>
      __routine;

  /// Argument to pass
  external ffi.Pointer<ffi.Void> __arg;

  external ffi.Pointer<__darwin_pthread_handler_rec> __next;
}

class _opaque_pthread_attr_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([56])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_cond_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([40])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_condattr_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_mutex_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([56])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_mutexattr_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_once_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_rwlock_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([192])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_rwlockattr_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int8> __opaque;
}

class _opaque_pthread_t extends ffi.Struct {
  @ffi.Int64()
  external int __sig;

  external ffi.Pointer<__darwin_pthread_handler_rec> __cleanup_stack;

  @ffi.Array.multi([8176])
  external ffi.Array<ffi.Int8> __opaque;
}

/// An enumeration to specify the type of a packet returned by the functions
abstract class NDIlib_frame_type_e {
  static const int NDIlib_frame_type_none = 0;
  static const int NDIlib_frame_type_video = 1;
  static const int NDIlib_frame_type_audio = 2;
  static const int NDIlib_frame_type_metadata = 3;
  static const int NDIlib_frame_type_error = 4;

  /// This indicates that the settings on this input have changed. For instance, this value will be returned
  /// from NDIlib_recv_capture_v2 and NDIlib_recv_capture when the device is known to have new settings, for
  /// instance the web URL has changed or the device is now known to be a PTZ camera.
  static const int NDIlib_frame_type_status_change = 100;

  /// Ensure that the size is 32bits
  static const int NDIlib_frame_type_max = 2147483647;
}

/// FourCC values for video frames
abstract class NDIlib_FourCC_video_type_e {
  static const int NDIlib_FourCC_video_type_UYVY = 1498831189;
  static const int NDIlib_FourCC_type_UYVY = 1498831189;

  /// YCbCr + Alpha color space, using 4:2:2:4.
  /// In memory there are two separate planes. The first is a regular
  /// UYVY 4:2:2 buffer. Immediately following this in memory is a
  /// alpha channel buffer.
  static const int NDIlib_FourCC_video_type_UYVA = 1096178005;
  static const int NDIlib_FourCC_type_UYVA = 1096178005;

  /// YCbCr color space using 4:2:2 in 16bpp
  /// In memory this is a semi-planar format. This is identical to a 16bpp version of the NV16 format.
  /// The first buffer is a 16bpp luminance buffer.
  /// Immediately after this is an interleaved buffer of 16bpp Cb, Cr pairs.
  static const int NDIlib_FourCC_video_type_P216 = 909193808;
  static const int NDIlib_FourCC_type_P216 = 909193808;

  /// YCbCr color space with an alpha channel, using 4:2:2:4
  /// In memory this is a semi-planar format.
  /// The first buffer is a 16bpp luminance buffer.
  /// Immediately after this is an interleaved buffer of 16bpp Cb, Cr pairs.
  /// Immediately after is a single buffer of 16bpp alpha channel.
  static const int NDIlib_FourCC_video_type_PA16 = 909197648;
  static const int NDIlib_FourCC_type_PA16 = 909197648;

  /// Planar 8bit 4:2:0 video format.
  /// The first buffer is an 8bpp luminance buffer.
  /// Immediately following this is a 8bpp Cr buffer.
  /// Immediately following this is a 8bpp Cb buffer.
  static const int NDIlib_FourCC_video_type_YV12 = 842094169;
  static const int NDIlib_FourCC_type_YV12 = 842094169;

  /// The first buffer is an 8bpp luminance buffer.
  /// Immediately following this is a 8bpp Cb buffer.
  /// Immediately following this is a 8bpp Cr buffer.
  static const int NDIlib_FourCC_video_type_I420 = 808596553;
  static const int NDIlib_FourCC_type_I420 = 808596553;

  /// Planar 8bit 4:2:0 video format.
  /// The first buffer is an 8bpp luminance buffer.
  /// Immediately following this is in interleaved buffer of 8bpp Cb, Cr pairs
  static const int NDIlib_FourCC_video_type_NV12 = 842094158;
  static const int NDIlib_FourCC_type_NV12 = 842094158;

  /// Planar 8bit, 4:4:4:4 video format.
  /// Color ordering in memory is blue, green, red, alpha
  static const int NDIlib_FourCC_video_type_BGRA = 1095911234;
  static const int NDIlib_FourCC_type_BGRA = 1095911234;

  /// Planar 8bit, 4:4:4 video format, packed into 32bit pixels.
  /// Color ordering in memory is blue, green, red, 255
  static const int NDIlib_FourCC_video_type_BGRX = 1481787202;
  static const int NDIlib_FourCC_type_BGRX = 1481787202;

  /// Planar 8bit, 4:4:4:4 video format.
  /// Color ordering in memory is red, green, blue, alpha
  static const int NDIlib_FourCC_video_type_RGBA = 1094862674;
  static const int NDIlib_FourCC_type_RGBA = 1094862674;

  /// Planar 8bit, 4:4:4 video format, packed into 32bit pixels.
  /// Color ordering in memory is red, green, blue, 255
  static const int NDIlib_FourCC_video_type_RGBX = 1480738642;
  static const int NDIlib_FourCC_type_RGBX = 1480738642;

  /// Ensure that the size is 32bits
  static const int NDIlib_FourCC_video_type_max = 2147483647;
}

/// FourCC values for audio frames
abstract class NDIlib_FourCC_audio_type_e {
  static const int NDIlib_FourCC_audio_type_FLTP = 1884572742;
  static const int NDIlib_FourCC_type_FLTP = 1884572742;

  /// Ensure that the size is 32bits
  static const int NDIlib_FourCC_audio_type_max = 2147483647;
}

abstract class NDIlib_frame_format_type_e {
  static const int NDIlib_frame_format_type_progressive = 1;

  /// A fielded frame with the field 0 being on the even lines and field 1 being
  /// on the odd lines/
  static const int NDIlib_frame_format_type_interleaved = 0;

  /// Individual fields
  static const int NDIlib_frame_format_type_field_0 = 2;
  static const int NDIlib_frame_format_type_field_1 = 3;

  /// Ensure that the size is 32bits
  static const int NDIlib_frame_format_type_max = 2147483647;
}

/// This is a descriptor of a NDI source available on the network.
class NDIlib_source_t extends ffi.Struct {
  /// etc... and comprises the machine name and the source name on that machine. In the form,
  /// MACHINE_NAME (NDI_SOURCE_NAME)
  /// If you specify this parameter either as NULL, or an EMPTY string then the specific IP address and port
  /// number from below is used.
  external ffi.Pointer<ffi.Int8> p_ndi_name;
}

/// This describes a video frame
class NDIlib_video_frame_v2_t extends ffi.Struct {
  @ffi.Int32()
  external int xres;

  @ffi.Int32()
  external int yres;

  /// What FourCC describing the type of data for this frame
  @ffi.Int32()
  external int FourCC;

  /// What is the frame-rate of this frame.
  /// For instance NTSC is 30000,1001 = 30000/1001 = 29.97fps
  @ffi.Int32()
  external int frame_rate_N;

  @ffi.Int32()
  external int frame_rate_D;

  /// What is the picture aspect ratio of this frame.
  /// For instance 16.0/9.0 = 1.778 is 16:9 video
  /// 0 means square pixels
  @ffi.Float()
  external double picture_aspect_ratio;

  /// Is this a fielded frame, or is it progressive
  @ffi.Int32()
  external int frame_format_type;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// The video data itself
  external ffi.Pointer<ffi.Uint8> p_data;

  /// Present in >= v2.5
  external ffi.Pointer<ffi.Int8> p_metadata;

  /// Present in >= v2.5
  @ffi.Int64()
  external int timestamp;
}

/// This describes an audio frame
class NDIlib_audio_frame_v2_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// The audio data
  external ffi.Pointer<ffi.Float> p_data;

  /// The inter channel stride of the audio channels, in bytes
  @ffi.Int32()
  external int channel_stride_in_bytes;

  /// Present in >= v2.5
  external ffi.Pointer<ffi.Int8> p_metadata;

  /// Present in >= v2.5
  @ffi.Int64()
  external int timestamp;
}

/// This describes an audio frame
class NDIlib_audio_frame_v3_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// What FourCC describing the type of data for this frame
  @ffi.Int32()
  external int FourCC;

  /// The audio data
  external ffi.Pointer<ffi.Uint8> p_data;

  /// Per frame metadata for this frame. This is a NULL terminated UTF8 string that should be in XML format.
  /// If you do not want any metadata then you may specify NULL here.
  external ffi.Pointer<ffi.Int8> p_metadata;

  /// This is only valid when receiving a frame and is specified as a 100ns time that was the exact moment
  /// that the frame was submitted by the sending side and is generated by the SDK. If this value is
  /// NDIlib_recv_timestamp_undefined then this value is not available and is NDIlib_recv_timestamp_undefined.
  @ffi.Int64()
  external int timestamp;
}

/// The data description for metadata
class NDIlib_metadata_frame_t extends ffi.Struct {
  /// 0, then the length is assume to be the length of a NULL terminated string.
  @ffi.Int32()
  external int length;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// The metadata as a UTF8 XML string. This is a NULL terminated string.
  external ffi.Pointer<ffi.Int8> p_data;
}

/// Tally structures
class NDIlib_tally_t extends ffi.Struct {
  @ffi.Uint8()
  external int on_program;

  /// Is this currently on preview output
  @ffi.Uint8()
  external int on_preview;
}

/// The creation structure that is used when you are creating a finder
class NDIlib_find_create_t extends ffi.Struct {
  /// local sources will be visible, if FALSE then they will not.
  @ffi.Uint8()
  external int show_local_sources;

  /// Which groups do you want to search in for sources
  external ffi.Pointer<ffi.Int8> p_groups;

  /// The list of additional IP addresses that exist that we should query for sources on. For instance, if
  /// you want to find the sources on a remote machine that is not on your local sub-net then you can put a
  /// comma separated list of those IP addresses here and those sources will be available locally even
  /// though they are not mDNS discoverable. An example might be "12.0.0.8,13.0.12.8". When none is
  /// specified the registry is used.
  /// Default = NULL;
  external ffi.Pointer<ffi.Int8> p_extra_ips;
}

/// Structures and type definitions required by NDI finding
/// The reference to an instance of the finder
typedef NDIlib_find_instance_t = ffi.Pointer<ffi.Void>;

abstract class NDIlib_recv_bandwidth_e {
  /// Receive metadata.
  static const int NDIlib_recv_bandwidth_metadata_only = -10;

  /// Receive metadata, audio.
  static const int NDIlib_recv_bandwidth_audio_only = 10;

  /// Receive metadata, audio, video at a lower bandwidth and resolution.
  static const int NDIlib_recv_bandwidth_lowest = 0;

  /// Receive metadata, audio, video at full resolution.
  static const int NDIlib_recv_bandwidth_highest = 100;

  /// Ensure this is 32bits in size
  static const int NDIlib_recv_bandwidth_max = 2147483647;
}

abstract class NDIlib_recv_color_format_e {
  /// When there is an alpha channel, this mode delivers BGRA
  static const int NDIlib_recv_color_format_BGRX_BGRA = 0;

  /// When there is no alpha channel, this mode delivers UYVY
  /// When there is an alpha channel, this mode delivers BGRA
  static const int NDIlib_recv_color_format_UYVY_BGRA = 1;

  /// When there is no alpha channel, this mode delivers BGRX
  /// When there is an alpha channel, this mode delivers RGBA
  static const int NDIlib_recv_color_format_RGBX_RGBA = 2;

  /// When there is no alpha channel, this mode delivers UYVY
  /// When there is an alpha channel, this mode delivers RGBA
  static const int NDIlib_recv_color_format_UYVY_RGBA = 3;

  /// This format will try to decode the video using the fastest available color format for the incoming
  /// video signal. This format follows the following guidelines, although different platforms might
  /// vary slightly based on their capabilities and specific performance profiles. In general if you want
  /// the best performance this mode should be used.
  ///
  /// When using this format, you should consider than allow_video_fields is true, and individual fields
  /// will always be delivered.
  ///
  /// For most video sources on most platforms, this will follow the following conventions
  /// No alpha channel : UYVY
  /// Alpha channel    : UYVA
  static const int NDIlib_recv_color_format_fastest = 100;

  /// This format will try to provide the video in the format that is the closest to native for the incoming
  /// codec yielding the highest quality. Specifically, this allows for receiving on 16bpp color from many
  /// sources.
  ///
  /// When using this format, you should consider than allow_video_fields is true, and individual fields
  /// will always be delivered.
  ///
  /// For most video sources on most platforms, this will follow the following conventions
  /// No alpha channel : P216, or UYVY
  /// Alpha channel    : PA16 or UYVA
  static const int NDIlib_recv_color_format_best = 101;

  /// Legacy definitions for backwards compatibility
  static const int NDIlib_recv_color_format_e_BGRX_BGRA = 0;
  static const int NDIlib_recv_color_format_e_UYVY_BGRA = 1;
  static const int NDIlib_recv_color_format_e_RGBX_RGBA = 2;
  static const int NDIlib_recv_color_format_e_UYVY_RGBA = 3;

  /// Force the size to be 32bits
  static const int NDIlib_recv_color_format_max = 2147483647;
}

/// The creation structure that is used when you are creating a receiver
class NDIlib_recv_create_v3_t extends ffi.Struct {
  external NDIlib_source_t source_to_connect_to;

  /// Your preference of color space. See above.
  @ffi.Int32()
  external int color_format;

  /// The bandwidth setting that you wish to use for this video source. Bandwidth controlled by changing
  /// both the compression level and the resolution of the source. A good use for low bandwidth is working
  /// on WIFI connections.
  @ffi.Int32()
  external int bandwidth;

  /// When this flag is FALSE, all video that you receive will be progressive. For sources that provide
  /// fields, this is de-interlaced on the receiving side (because we cannot change what the up-stream
  /// source was actually rendering. This is provided as a convenience to down-stream sources that do not
  /// wish to understand fielded video. There is almost no  performance impact of using this function.
  @ffi.Uint8()
  external int allow_video_fields;

  /// The name of the NDI receiver to create. This is a NULL terminated UTF8 string and should be the name
  /// of receive channel that you have. This is in many ways symmetric with the name of senders, so this
  /// might be "Channel 1" on your system. If this is NULL then it will use the filename of your application
  /// indexed with the number of the instance number of this receiver.
  external ffi.Pointer<ffi.Int8> p_ndi_recv_name;
}

/// This allows you determine the current performance levels of the receiving to be able to detect whether
/// frames have been dropped.
class NDIlib_recv_performance_t extends ffi.Struct {
  @ffi.Int64()
  external int video_frames;

  /// The number of audio frames
  @ffi.Int64()
  external int audio_frames;

  /// The number of metadata frames
  @ffi.Int64()
  external int metadata_frames;
}

/// Get the current queue depths
class NDIlib_recv_queue_t extends ffi.Struct {
  @ffi.Int32()
  external int video_frames;

  /// The number of audio frames
  @ffi.Int32()
  external int audio_frames;

  /// The number of metadata frames
  @ffi.Int32()
  external int metadata_frames;
}

/// Structures and type definitions required by NDI finding
/// The reference to an instance of the receiver
typedef NDIlib_recv_instance_t = ffi.Pointer<ffi.Void>;

/// In order to get the duration
class NDIlib_recv_recording_time_t extends ffi.Struct {
  @ffi.Int64()
  external int no_frames;

  /// The starting time and current largest time of the record, in UTC time, at 100ns unit intervals. This
  /// allows you to know the record time irrespective of frame-rate. For instance, last_time - start_time
  /// would give you the recording length in 100ns intervals.
  @ffi.Int64()
  external int start_time;

  @ffi.Int64()
  external int last_time;
}

/// The creation structure that is used when you are creating a sender
class NDIlib_send_create_t extends ffi.Struct {
  external ffi.Pointer<ffi.Int8> p_ndi_name;

  /// What groups should this source be part of. NULL means default.
  external ffi.Pointer<ffi.Int8> p_groups;

  /// Do you want audio and video to "clock" themselves. When they are clocked then by adding video frames,
  /// they will be rate limited to match the current frame-rate that you are submitting at. The same is true
  /// for audio. In general if you are submitting video and audio off a single thread then you should only
  /// clock one of them (video is probably the better of the two to clock off). If you are submitting audio
  /// and video of separate threads then having both clocked can be useful.
  @ffi.Uint8()
  external int clock_video;

  @ffi.Uint8()
  external int clock_audio;
}

/// Structures and type definitions required by NDI sending
/// The reference to an instance of the sender
typedef NDIlib_send_instance_t = ffi.Pointer<ffi.Void>;

/// The creation structure that is used when you are creating a sender
class NDIlib_routing_create_t extends ffi.Struct {
  external ffi.Pointer<ffi.Int8> p_ndi_name;

  /// What groups should this source be part of
  external ffi.Pointer<ffi.Int8> p_groups;
}

/// Structures and type definitions required by NDI routing.
/// The reference to an instance of the router.
typedef NDIlib_routing_instance_t = ffi.Pointer<ffi.Void>;

/// This describes an audio frame
class NDIlib_audio_frame_interleaved_16s_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels.
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel.
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals.
  @ffi.Int64()
  external int timecode;

  /// The audio reference level in dB. This specifies how many dB above the reference level (+4dBU) is the
  /// full range of 16 bit audio. If you do not understand this and want to just use numbers:
  /// - If you are sending audio, specify +0dB. Most common applications produce audio at reference level.
  /// - If receiving audio, specify +20dB. This means that the full 16 bit range corresponds to professional
  /// level audio with 20dB of headroom. Note that if you are writing it into a file it might sound soft
  /// because you have 20dB of headroom before clipping.
  @ffi.Int32()
  external int reference_level;

  /// The audio data, interleaved 16bpp.
  external ffi.Pointer<ffi.Int16> p_data;
}

/// This describes an audio frame.
class NDIlib_audio_frame_interleaved_32s_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels.
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel.
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals.
  @ffi.Int64()
  external int timecode;

  /// The audio reference level in dB. This specifies how many dB above the reference level (+4dBU) is the
  /// full range of 16 bit audio. If you do not understand this and want to just use numbers:
  /// - If you are sending audio, specify +0dB. Most common applications produce audio at reference level.
  /// - If receiving audio, specify +20dB. This means that the full 16 bit range corresponds to professional
  /// level audio with 20dB of headroom. Note that if you are writing it into a file it might sound soft
  /// because you have 20dB of headroom before clipping.
  @ffi.Int32()
  external int reference_level;

  /// The audio data, interleaved 32bpp.
  external ffi.Pointer<ffi.Int32> p_data;
}

/// This describes an audio frame.
class NDIlib_audio_frame_interleaved_32f_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels.
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel.
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals.
  @ffi.Int64()
  external int timecode;

  /// The audio data, interleaved 32bpp.
  external ffi.Pointer<ffi.Float> p_data;
}

/// This describes a video frame
class NDIlib_video_frame_t extends ffi.Struct {
  @ffi.Int32()
  external int xres;

  @ffi.Int32()
  external int yres;

  /// What FourCC this is with. This can be two values
  @ffi.Int32()
  external int FourCC;

  /// What is the frame-rate of this frame.
  /// For instance NTSC is 30000,1001 = 30000/1001 = 29.97fps
  @ffi.Int32()
  external int frame_rate_N;

  @ffi.Int32()
  external int frame_rate_D;

  /// What is the picture aspect ratio of this frame.
  /// For instance 16.0/9.0 = 1.778 is 16:9 video. If this is zero, then square pixels are assumed (xres/yres)
  @ffi.Float()
  external double picture_aspect_ratio;

  /// Is this a fielded frame, or is it progressive
  @ffi.Int32()
  external int frame_format_type;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// The video data itself
  external ffi.Pointer<ffi.Uint8> p_data;

  /// The inter line stride of the video data, in bytes.
  @ffi.Int32()
  external int line_stride_in_bytes;
}

/// This describes an audio frame
class NDIlib_audio_frame_t extends ffi.Struct {
  @ffi.Int32()
  external int sample_rate;

  /// The number of audio channels
  @ffi.Int32()
  external int no_channels;

  /// The number of audio samples per channel
  @ffi.Int32()
  external int no_samples;

  /// The timecode of this frame in 100ns intervals
  @ffi.Int64()
  external int timecode;

  /// The audio data
  external ffi.Pointer<ffi.Float> p_data;

  /// The inter channel stride of the audio channels, in bytes
  @ffi.Int32()
  external int channel_stride_in_bytes;
}

/// The creation structure that is used when you are creating a receiver
class NDIlib_recv_create_t extends ffi.Struct {
  external NDIlib_source_t source_to_connect_to;

  /// Your preference of color space. See above.
  @ffi.Int32()
  external int color_format;

  /// The bandwidth setting that you wish to use for this video source. Bandwidth
  /// controlled by changing both the compression level and the resolution of the source.
  /// A good use for low bandwidth is working on WIFI connections.
  @ffi.Int32()
  external int bandwidth;

  /// When this flag is FALSE, all video that you receive will be progressive. For sources that provide
  /// fields, this is de-interlaced on the receiving side (because we cannot change what the up-stream
  /// source was actually rendering. This is provided as a convenience to down-stream sources that do not
  /// wish to understand fielded video. There is almost no performance impact of using this function.
  @ffi.Uint8()
  external int allow_video_fields;
}

/// The type instance for a frame-synchronizer
typedef NDIlib_framesync_instance_t = ffi.Pointer<ffi.Void>;

/// NOTE : The following MIT license applies to this file ONLY and not to the SDK as a whole. Please review
/// the SDK documentation for the description of the full license terms, which are also provided in the file
/// "NDI License Agreement.pdf" within the SDK or online at http://new.tk/ndisdk_license/. Your use of any
/// part of this SDK is acknowledgment that you agree to the SDK license terms. The full NDI SDK may be
/// downloaded at http://ndi.tv/
///
/// ***********************************************************************************************************
///
/// Copyright (C)2014-2021, NewTek, inc.
///
/// Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
/// associated documentation files(the "Software"), to deal in the Software without restriction, including
/// without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and / or sell
/// copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the
/// following conditions :
///
/// The above copyright notice and this permission notice shall be included in all copies or substantial
/// portions of the Software.
///
/// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
/// LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO
/// EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
/// IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR
/// THE USE OR OTHER DEALINGS IN THE SOFTWARE.
///
/// ***********************************************************************************************************
class NDIlib_v5 extends ffi.Opaque {}

typedef NDIlib_v4_5 = NDIlib_v5;
typedef NDIlib_v4 = NDIlib_v5;
typedef NDIlib_v3 = NDIlib_v5;
typedef NDIlib_v2 = NDIlib_v5;

const String NDILIB_LIBRARY_NAME = 'libndi.dylib';

const String NDILIB_REDIST_FOLDER = 'NDI_RUNTIME_DIR_V5';

const String NDILIB_REDIST_URL = 'http://new.tk/NDIRedistV5Apple';

const int NDILIB_CPP_DEFAULT_CONSTRUCTORS = 1;

const int true1 = 1;

const int false1 = 0;

const int __WORDSIZE = 64;

const int __DARWIN_ONLY_64_BIT_INO_T = 0;

const int __DARWIN_ONLY_UNIX_CONFORMANCE = 1;

const int __DARWIN_ONLY_VERS_1050 = 0;

const int __DARWIN_UNIX03 = 1;

const int __DARWIN_64_BIT_INO_T = 1;

const int __DARWIN_VERS_1050 = 1;

const int __DARWIN_NON_CANCELABLE = 0;

const String __DARWIN_SUF_64_BIT_INO_T = '\$INODE64';

const String __DARWIN_SUF_1050 = '\$1050';

const String __DARWIN_SUF_EXTSN = '\$DARWIN_EXTSN';

const int __DARWIN_C_ANSI = 4096;

const int __DARWIN_C_FULL = 900000;

const int __DARWIN_C_LEVEL = 900000;

const int __STDC_WANT_LIB_EXT1__ = 1;

const int __DARWIN_NO_LONG_LONG = 0;

const int _DARWIN_FEATURE_64_BIT_INODE = 1;

const int _DARWIN_FEATURE_ONLY_UNIX_CONFORMANCE = 1;

const int _DARWIN_FEATURE_UNIX_CONFORMANCE = 3;

const int __has_ptrcheck = 0;

const int __DARWIN_NULL = 0;

const int __PTHREAD_SIZE__ = 8176;

const int __PTHREAD_ATTR_SIZE__ = 56;

const int __PTHREAD_MUTEXATTR_SIZE__ = 8;

const int __PTHREAD_MUTEX_SIZE__ = 56;

const int __PTHREAD_CONDATTR_SIZE__ = 8;

const int __PTHREAD_COND_SIZE__ = 40;

const int __PTHREAD_ONCE_SIZE__ = 8;

const int __PTHREAD_RWLOCK_SIZE__ = 192;

const int __PTHREAD_RWLOCKATTR_SIZE__ = 16;

const int USER_ADDR_NULL = 0;

const int INT8_MAX = 127;

const int INT16_MAX = 32767;

const int INT32_MAX = 2147483647;

const int INT64_MAX = 9223372036854775807;

const int INT8_MIN = -128;

const int INT16_MIN = -32768;

const int INT32_MIN = -2147483648;

const int INT64_MIN = -9223372036854775808;

const int UINT8_MAX = 255;

const int UINT16_MAX = 65535;

const int UINT32_MAX = 4294967295;

const int UINT64_MAX = -1;

const int INT_LEAST8_MIN = -128;

const int INT_LEAST16_MIN = -32768;

const int INT_LEAST32_MIN = -2147483648;

const int INT_LEAST64_MIN = -9223372036854775808;

const int INT_LEAST8_MAX = 127;

const int INT_LEAST16_MAX = 32767;

const int INT_LEAST32_MAX = 2147483647;

const int INT_LEAST64_MAX = 9223372036854775807;

const int UINT_LEAST8_MAX = 255;

const int UINT_LEAST16_MAX = 65535;

const int UINT_LEAST32_MAX = 4294967295;

const int UINT_LEAST64_MAX = -1;

const int INT_FAST8_MIN = -128;

const int INT_FAST16_MIN = -32768;

const int INT_FAST32_MIN = -2147483648;

const int INT_FAST64_MIN = -9223372036854775808;

const int INT_FAST8_MAX = 127;

const int INT_FAST16_MAX = 32767;

const int INT_FAST32_MAX = 2147483647;

const int INT_FAST64_MAX = 9223372036854775807;

const int UINT_FAST8_MAX = 255;

const int UINT_FAST16_MAX = 65535;

const int UINT_FAST32_MAX = 4294967295;

const int UINT_FAST64_MAX = -1;

const int INTPTR_MAX = 9223372036854775807;

const int INTPTR_MIN = -9223372036854775808;

const int UINTPTR_MAX = -1;

const int INTMAX_MAX = 9223372036854775807;

const int UINTMAX_MAX = -1;

const int INTMAX_MIN = -9223372036854775808;

const int PTRDIFF_MIN = -9223372036854775808;

const int PTRDIFF_MAX = 9223372036854775807;

const int SIZE_MAX = -1;

const int RSIZE_MAX = 9223372036854775807;

const int WCHAR_MAX = 2147483647;

const int WCHAR_MIN = -2147483648;

const int WINT_MIN = -2147483648;

const int WINT_MAX = 2147483647;

const int SIG_ATOMIC_MIN = -2147483648;

const int SIG_ATOMIC_MAX = 2147483647;
